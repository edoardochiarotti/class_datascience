{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d3011f9",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/edoardochiarotti/class_datascience/blob/main/2024/07_Endogeneity/07_Endogeneity_exercises_solutions.ipynb\"\n",
    "   target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b51f57",
   "metadata": {},
   "source": [
    "# Endogeneity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d486ef7a",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgflip.com/84g5gd.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388c0f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGES\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random as rd\n",
    "import statistics as st\n",
    "import pandas as pd\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "import re\n",
    "from stargazer.stargazer import Stargazer\n",
    "from linearmodels.panel import PanelOLS # conda install -c conda-forge linearmodels\n",
    "\n",
    "# FUNCTIONS FROM PACKAGES\n",
    "from numpy.linalg import inv\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# SEABORN THEME\n",
    "scale = 0.4\n",
    "W = 16*scale\n",
    "H = 9*scale\n",
    "sns.set(rc = {'figure.figsize':(W,H)})\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f66d42b",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- [Exercise 1: Multivariate Linear Regression Model](#Exercise-1:-Multivariate-Linear-Regression-Model)\n",
    "- [Exercise 2: Interaction Terms](#Exercise-2:-Interaction-Terms)\n",
    "- [Exercise 3: Fixed Effects](#Exercise-3:-Fixed-Effects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142cf6b",
   "metadata": {},
   "source": [
    "## Exercise 1: Multivariate Linear Regression Model <a name=\"Exercise-1:-Multivariate-Linear-Regression-Model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b94d7",
   "metadata": {},
   "source": [
    "- Get data on CO2 emissions per capita, income per capita and climate-related tax revenues as done in class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b341592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "link = \"https://www.qogdata.pol.gu.se/data/qog_ei_sept21.xlsx\"\n",
    "df_qog = pd.read_excel(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a42738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get variables\n",
    "indexes = [\"ccodealp\",\"year\"]\n",
    "variabs_co2 = [\"edgar_co2gdp\",\"edgar_co2t\",\"edgar_co2pc\"]\n",
    "variabs_control = [\"oecd_cctr_gdp\"]\n",
    "variabs = variabs_co2 + variabs_control\n",
    "df = df_qog.loc[:,np.append(indexes,variabs)]\n",
    "\n",
    "# make gdp per capita\n",
    "df[\"gdp\"] = (df[\"edgar_co2gdp\"]/df[\"edgar_co2t\"])**(-1) # billions US dollars\n",
    "df[\"pop\"] = (df[\"edgar_co2pc\"]/df[\"edgar_co2t\"])**(-1) # millions\n",
    "df[\"gdp_pc\"] = df[\"gdp\"]/df[\"pop\"] # thousands of US dollars\n",
    "variabs = np.append(variabs, [\"gdp\",\"pop\",\"gdp_pc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62451750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make cross section\n",
    "df_cross = df.groupby(\"ccodealp\")[variabs].mean().reset_index().dropna()\n",
    "\n",
    "# put ones into data\n",
    "df_cross[\"ones\"] = 1\n",
    "\n",
    "# drop outliers quick and dirty\n",
    "df_cross = df_cross.loc[df_cross[\"gdp_pc\"] < 80,:]\n",
    "\n",
    "# maybe logs?\n",
    "df_cross[\"ln_gdp_pc\"] = np.log(df_cross[\"gdp_pc\"])\n",
    "df_cross[\"ln_edgar_co2pc\"] = np.log(df_cross[\"edgar_co2pc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0052b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# canned ols\n",
    "ols_canned_results = sm.OLS.from_formula('ln_edgar_co2pc ~ ln_gdp_pc + oecd_cctr_gdp', df_cross).fit()\n",
    "ols_canned_results_table = ols_canned_results.summary().tables[1]\n",
    "ols_canned_results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f217f3ea",
   "metadata": {},
   "source": [
    "- Use the functions you have written in the exercise Notebook of week 6, called `data_to_matrix` and `OLS_estimator`, to estimate this table by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e263f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ...\n",
    "\n",
    "# define our functions\n",
    "\n",
    "# function to transform panda series into vectors / matrices\n",
    "def data_to_matrix(data, variab_name):\n",
    "    \n",
    "    \"\"\" My Data to Matrix Function \"\"\"\n",
    "    \n",
    "    # store in matrixes\n",
    "    matrix = data.loc[:,variab_name].to_numpy()\n",
    "    \n",
    "    # make column vectors for arrays with less than 2 dimensions\n",
    "    if len(matrix.shape) == 1:\n",
    "        matrix = np.atleast_2d(matrix).T\n",
    "        \n",
    "    # return result\n",
    "    return matrix\n",
    "\n",
    "# ols function\n",
    "def OLS_estimator(data, y, X):\n",
    "    \n",
    "    \"\"\" My Sample Mean Function \"\"\"\n",
    "    \n",
    "    # store in matrixes\n",
    "    ydata = data_to_matrix(data, variab_name = y)\n",
    "    xdata = data_to_matrix(data, variab_name = X)\n",
    "    \n",
    "    # get params\n",
    "    N = len(ydata)\n",
    "    K = xdata.shape[1]\n",
    "    DF = N-K\n",
    "\n",
    "    # get OLS estimate\n",
    "    betahat_OLS = (inv(xdata.T @ xdata)) @ (xdata.T @ ydata)\n",
    "    \n",
    "    # get standard errors\n",
    "    resid_OLS = (ydata - xdata @ betahat_OLS)\n",
    "    sigma2hat_OLS = (resid_OLS.T @ resid_OLS) / DF\n",
    "    betahat_OLS_vcov = sigma2hat_OLS * inv(xdata.T @ xdata)\n",
    "    sehat_OLS = np.atleast_2d(np.sqrt(betahat_OLS_vcov.diagonal())).T\n",
    "    \n",
    "    # get t stat\n",
    "    t_stat_OLS = betahat_OLS / sehat_OLS\n",
    "    \n",
    "    # get p values and confidence intervals\n",
    "    \n",
    "    # create objects to store results\n",
    "    p_values_OLS = np.empty((K,1,))\n",
    "    ci_low_OLS = np.empty((K,1,))\n",
    "    ci_high_OLS = np.empty((K,1,))\n",
    "    \n",
    "    # run loop\n",
    "    for i in range(K):\n",
    "        \n",
    "        # get p value\n",
    "        lower_area = stats.t.cdf(-abs(float(t_stat_OLS[i])), df = DF)\n",
    "        upper_area = lower_area\n",
    "        p_value = lower_area + upper_area\n",
    "        p_values_OLS[i] = p_value\n",
    "\n",
    "        # get confidence interval\n",
    "        alpha_inv = (1.0-0.05)\n",
    "        q1 = (1+alpha_inv)/2\n",
    "        ci_critical = stats.t.ppf(q1, DF)\n",
    "        ci_low_OLS[i] = betahat_OLS[i]-(ci_critical*sehat_OLS[i])\n",
    "        ci_high_OLS[i] = betahat_OLS[i]+(ci_critical*sehat_OLS[i])\n",
    "\n",
    "    # get table\n",
    "    df_res = pd.DataFrame(index=np.arange(K), columns=np.arange(7))\n",
    "    df_res.columns = [\"variable\",\"coef\",\"std err\",\"t\",\"P>|t|\",\"[0.025\",\"0.975]\"]\n",
    "    df_res[\"variable\"] = X\n",
    "    df_res[\"coef\"] = np.around(betahat_OLS, 4)\n",
    "    df_res[\"std err\"] = np.around(sehat_OLS, 3)\n",
    "    df_res[\"t\"] = np.around(t_stat_OLS, 3)\n",
    "    df_res[\"P>|t|\"] = np.around(p_values_OLS, 3)\n",
    "    df_res[\"[0.025\"] = np.around(ci_low_OLS, 3)\n",
    "    df_res[\"0.975]\"] = np.around(ci_high_OLS, 3)\n",
    "\n",
    "    # return\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87163b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try\n",
    "OLS_estimator(data = df_cross, y = \"ln_edgar_co2pc\", X = [\"ones\",\"ln_gdp_pc\",\"oecd_cctr_gdp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec3105",
   "metadata": {},
   "source": [
    "- Our functions should work as we've been good! Note that usually the routines in Stata (and Python I think) for multivariate regressions use the Frisch-Waugh-Lovell Theorem to estimate the single regressors' coefficients. This is much more efficient when the number of regressors become high, or when we have fixed effects. We will not cover it in this class, but you should check the material on [Partitioned Regression](https://static1.squarespace.com/static/558eff8ce4b023b6b855320a/t/573162b320c6478f4961c5bb/1462854326527/ARE_212_Section_4.pdf) by Fiona Burlig, who shows how to do this in R in details and explains why this theorem is so important. We have used it in the class of fixed effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d41643",
   "metadata": {},
   "source": [
    "## Exercise 2: Interaction Terms <a name=\"Exercise-2:-Interaction-Terms\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dde9b65",
   "metadata": {},
   "source": [
    "- Use the functions written above (not canned routines) to estimate the log-log model with the interaction term between income per capita (`ln_gdp_pc`) and climate change-related tax revenue (`oecd_cctr_gdp`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300930b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# create interaction\n",
    "df_cross[\"ln_gdp_pc_times_oecd_cctr_gdp\"] = df_cross[\"ln_gdp_pc\"]*df_cross[\"oecd_cctr_gdp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by hand\n",
    "OLS_estimator(data = df_cross, y = \"ln_edgar_co2pc\", X = [\"ones\",\"ln_gdp_pc\",\"oecd_cctr_gdp\",\n",
    "                                                    \"ln_gdp_pc_times_oecd_cctr_gdp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e0edf9",
   "metadata": {},
   "source": [
    "- Use the functions written here to estimate the level-level model with the interaction term between income per capita (`gdp_pc`) and a dummy variable which equals 1 for countries with high climate change-related tax revenues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3304c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# create the variable\n",
    "median = df_cross['oecd_cctr_gdp'].median()\n",
    "df_cross['dummy_oecd_cctr_gdp'] = df_cross['oecd_cctr_gdp'] >= median\n",
    "df_cross[\"dummy_oecd_cctr_gdp\"] = df_cross[\"dummy_oecd_cctr_gdp\"].astype(int)\n",
    "\n",
    "# create interaction\n",
    "df_cross[\"gdp_pc_times_dummy_oecd_cctr_gdp\"] = df_cross[\"gdp_pc\"]*df_cross[\"dummy_oecd_cctr_gdp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e7f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by hand\n",
    "OLS_estimator(data = df_cross, y = \"edgar_co2pc\", X = [\"ones\",\"gdp_pc\",\"dummy_oecd_cctr_gdp\",\n",
    "                                                    \"gdp_pc_times_dummy_oecd_cctr_gdp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9ba630",
   "metadata": {},
   "source": [
    "- Comment the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6ad2d9",
   "metadata": {},
   "source": [
    "- Your answer here: ... The effect of income on emissions is lower in countries with high climate-related taxes. Specifically, the income-induced increase in CO2 emissions per capita is 0.09 tonnes lower in countries above median of climate change-related income revenues (compared to countries below median). This result is statistically significant at the 10% level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519dbe17",
   "metadata": {},
   "source": [
    "## Exercise 3: Fixed Effects <a name=\"Exercise-3:-Fixed-Effects\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d91bb69",
   "metadata": {},
   "source": [
    "- Let's take a subset of the full panel data loaded above, as done in class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171fa748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nas\n",
    "df_panel = df.dropna()\n",
    "\n",
    "# from 2010 to 2016\n",
    "df_panel = df_panel.loc[(df_panel[\"year\"] >= 2010) & (df_panel[\"year\"] <= 2017),:]\n",
    "\n",
    "# drop outliers quick and dirty\n",
    "df_panel = df_panel.loc[(df_panel[\"gdp_pc\"] < 65) & (df_panel[\"edgar_co2pc\"] < 20),:]\n",
    "\n",
    "# lets make it balanced for simplicity\n",
    "df_panel_agg = df_panel.groupby(['year']).size().reset_index(name='counts')\n",
    "year_min = df_panel_agg.loc[df_panel_agg.counts == min(df_panel_agg.counts),\"year\"].values\n",
    "ccodealp_sub = np.unique(df_panel.loc[df_panel.year == int(year_min),\"ccodealp\"].values)\n",
    "df_panel = df_panel.loc[np.isin(df_panel.ccodealp, ccodealp_sub),:]\n",
    "\n",
    "df_panel_agg = df_panel.groupby(['ccodealp']).size().reset_index(name='counts')\n",
    "cou_drop = df_panel_agg.loc[df_panel_agg.counts == min(np.unique(df_panel_agg[\"counts\"])),\"ccodealp\"].values\n",
    "df_panel = df_panel.loc[~np.isin(df_panel.ccodealp, cou_drop),:]\n",
    "\n",
    "# maybe logs?\n",
    "df_panel[\"ln_gdp_pc\"] = np.log(df_panel[\"gdp_pc\"])\n",
    "df_panel[\"ln_edgar_co2pc\"] = np.log(df_panel[\"edgar_co2pc\"])\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(x='ln_gdp_pc', y='ln_edgar_co2pc', data=df_panel, color = \"r\", s = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9f02bd",
   "metadata": {},
   "source": [
    "- Create a function called `FE_estimator` by updating the function used above (called `OLS_estimator`) with the equations seen in class to estimate the model parameters with the fixed-effect estimator. Some tips:\n",
    "    - Instead of the usual expression for the OLS estimator, you should have something like this: `beta_hat_FE = (inv(xdata_dem.T @ xdata_dem)) @ (xdata_dem.T @ ydata_dem)`\n",
    "    - To demean the data, you need to define the matrixes $\\iota_T$, $P_{NT}$, etc\n",
    "    - Note that we you need to add an extra $N$ in the degrees of freedom correction, because all of this is as if we were estimating also $N$ country dummies (even if we don't effectively do it, we can't cheat statistics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1002f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ...\n",
    "\n",
    "# FE function\n",
    "def FE_estimator(data, y, X, indexes):\n",
    "    \n",
    "    \"\"\" My Sample Mean Function \"\"\"\n",
    "    \n",
    "    # store in matrixes\n",
    "    ydata = data_to_matrix(data, variab_name = y)\n",
    "    xdata = data_to_matrix(data, variab_name = X)\n",
    "    \n",
    "    # get params\n",
    "    N = len(data[indexes[0]].unique())\n",
    "    T = len(data[indexes[1]].unique())\n",
    "    K = xdata.shape[1]\n",
    "    DF = N*T-K-N\n",
    "    \n",
    "    # demean\n",
    "    iota = np.atleast_2d(np.ones(T)).T\n",
    "    P = np.kron(np.identity(N), 1/T * iota @ iota.T)\n",
    "    Q = np.identity(N*T) - P\n",
    "    ydata_dem = Q @ ydata\n",
    "    xdata_dem = Q @ xdata\n",
    "\n",
    "    # get OLS estimate\n",
    "    beta_hat_FE = (inv(xdata_dem.T @ xdata_dem)) @ (xdata_dem.T @ ydata_dem)\n",
    "    \n",
    "    # get standard errors\n",
    "    resid_FE = (ydata_dem - xdata_dem @ beta_hat_FE)\n",
    "    sigma2_hat_FE = (resid_FE.T @ resid_FE) / DF\n",
    "    beta_hat_FE_vcov = sigma2_hat_FE * inv(xdata_dem.T @ xdata_dem)\n",
    "    se_hat_FE = np.atleast_2d(np.sqrt(beta_hat_FE_vcov.diagonal())).T\n",
    "    \n",
    "    # get t stat\n",
    "    t_stat_FE = beta_hat_FE / se_hat_FE\n",
    "    \n",
    "    # get p values and confidence intervals\n",
    "    \n",
    "    # create objects to store results\n",
    "    p_values_FE = np.empty((K,1,))\n",
    "    ci_low_FE = np.empty((K,1,))\n",
    "    ci_high_FE = np.empty((K,1,))\n",
    "    \n",
    "    # run loop\n",
    "    for i in range(K):\n",
    "        \n",
    "        # get p value\n",
    "        lower_area = stats.t.cdf(-abs(float(t_stat_FE[i])), df = DF)\n",
    "        upper_area = lower_area\n",
    "        p_value = lower_area + upper_area\n",
    "        p_values_FE[i] = p_value\n",
    "\n",
    "        # get confidence interval\n",
    "        alpha_inv = (1.0-0.05)\n",
    "        q1 = (1+alpha_inv)/2\n",
    "        ci_critical = stats.t.ppf(q1, DF)\n",
    "        ci_low_FE[i] = beta_hat_FE[i]-(ci_critical*se_hat_FE[i])\n",
    "        ci_high_FE[i] = beta_hat_FE[i]+(ci_critical*se_hat_FE[i])\n",
    "\n",
    "    # get table\n",
    "    df_res = pd.DataFrame(index=np.arange(K), columns=np.arange(7))\n",
    "    df_res.columns = [\"variable\",\"coef\",\"std err\",\"t\",\"P>|t|\",\"[0.025\",\"0.975]\"]\n",
    "    df_res[\"variable\"] = X\n",
    "    df_res[\"coef\"] = np.around(beta_hat_FE, 4)\n",
    "    df_res[\"std err\"] = np.around(se_hat_FE, 4)\n",
    "    df_res[\"t\"] = np.around(t_stat_FE, 4)\n",
    "    df_res[\"P>|t|\"] = np.around(p_values_FE, 4)\n",
    "    df_res[\"[0.025\"] = np.around(ci_low_FE, 4)\n",
    "    df_res[\"0.975]\"] = np.around(ci_high_FE, 4)\n",
    "\n",
    "    # return\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707249cb",
   "metadata": {},
   "source": [
    "- Test this function by estimating the log-log model that regresses the log of CO2 emissions per capita on the log of GDP per capita and the climate change-related tax revenues, using the panel database built above, and comment the coefficient estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23af9b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ...\n",
    "\n",
    "# our function\n",
    "FE_estimator(data = df_panel, y = [\"ln_edgar_co2pc\"], X = [\"ln_gdp_pc\",\"oecd_cctr_gdp\"], \n",
    "             indexes = [\"ccodealp\",\"year\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac61fff7",
   "metadata": {},
   "source": [
    "- Your answer here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e96f631",
   "metadata": {},
   "source": [
    "- Test that the output is the same of the one obtained in class with the canned routine `PanelOLS.from_formula`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08853bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ...\n",
    "\n",
    "# Canned method\n",
    "est_fe_canned = PanelOLS.from_formula(\"ln_edgar_co2pc ~ ln_gdp_pc + oecd_cctr_gdp + EntityEffects\",\n",
    "                            data=df_panel.set_index([\"ccodealp\", \"year\"]))\n",
    "result = est_fe_canned.fit()\n",
    "result.summary.tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5946fdf8",
   "metadata": {},
   "source": [
    "- As done in class, compute the demeaned panel database (we called it `demeaned_df`) and use the OLS canned routine `sm.OLS.from_formula` on this database to test if it gives you the same results of your function `FE_estimator` (careful about the intercept). Are the results the same or different? And if they differ, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f89a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ...\n",
    "\n",
    "# compute means by country\n",
    "y = \"ln_edgar_co2pc\"\n",
    "X = [\"ln_gdp_pc\",\"oecd_cctr_gdp\"]\n",
    "mean_df_panel = df_panel.groupby(\"ccodealp\")[[y] + X].mean()\n",
    "\n",
    "# compute demeaned df\n",
    "demeaned_df = (df_panel\n",
    "               .set_index(\"ccodealp\") # set the index as the person indicator\n",
    "               [[y] + X]\n",
    "               - mean_df_panel) # subtract the mean data\n",
    "\n",
    "# canned method\n",
    "est_fe = sm.OLS.from_formula(\"ln_edgar_co2pc ~ ln_gdp_pc + oecd_cctr_gdp - 1\", data=demeaned_df).fit()\n",
    "est_fe.summary().tables[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
