{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c9ed54c",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/edoardochiarotti/class_datascience/blob/main/2024/06_Linear-Regression-Model/06_Linear_regression_model_exercises.ipynb\"\n",
    "   target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e851a0",
   "metadata": {},
   "source": [
    "# The Linear Regression Model - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c650083",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgflip.com/83mjm6.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dc29c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGES\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random as rd\n",
    "import statistics as st\n",
    "import pandas as pd\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "import re\n",
    "\n",
    "# FUNCTIONS FROM PACKAGES\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# SEABORN THEME\n",
    "scale = 0.4\n",
    "W = 16*scale\n",
    "H = 9*scale\n",
    "sns.set(rc = {'figure.figsize':(W,H)})\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d105087",
   "metadata": {},
   "source": [
    "## Content\n",
    "- [Exercise 1: Function for OLS coefficients](#Exercise-1:-Function-for-OLS-coefficients)\n",
    "- [Exercise 2: Make a nice output table](#Exercise-2:-Make-a-nice-output-table)\n",
    "- [Exercise 3: Add statistics to the output table](#Exercise-3:-Add-statistics-to-the-output-table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a7bf1a",
   "metadata": {},
   "source": [
    "- As done in class, let's consider the relationship between CO2 emissions per capita and income per capita.\n",
    "- So, instead of assuming that the mean component of CO2 emissions per capita is simply $\\beta$, we'll assume that its mean component is $\\beta_0+\\beta_1x$, where $x$ is GDP per capita. In other words, we assume that, on average, CO2 emissions per capita linearly depend on the value of the GDP per capita. Or similarly, that we can use GDP per capita to predict CO2 emissions per capita.\n",
    "- Let's get **QoG** and add the variables as done in class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3228eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "link = \"https://www.qogdata.pol.gu.se/data/qog_ei_sept21.xlsx\"\n",
    "df_qog = pd.read_excel(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a041b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get variables\n",
    "indexes = [\"ccodealp\",\"year\"]\n",
    "variabs_co2 = [\"edgar_co2gdp\",\"edgar_co2t\",\"edgar_co2pc\"]\n",
    "variabs_control = [\"oecd_cctr_gdp\"]\n",
    "variabs = variabs_co2 + variabs_control\n",
    "df = df_qog.loc[:,np.append(indexes,variabs)]\n",
    "\n",
    "# make gdp per capita\n",
    "df[\"gdp\"] = (df[\"edgar_co2gdp\"]/df[\"edgar_co2t\"])**(-1) # billions US dollars\n",
    "df[\"pop\"] = (df[\"edgar_co2pc\"]/df[\"edgar_co2t\"])**(-1) # millions\n",
    "df[\"gdp_pc\"] = df[\"gdp\"]/df[\"pop\"] # thousands of US dollars\n",
    "variabs = np.append(variabs, [\"gdp\",\"pop\",\"gdp_pc\"])\n",
    "\n",
    "# make cross section\n",
    "df = df.groupby(\"ccodealp\")[variabs].mean().reset_index().dropna()\n",
    "\n",
    "# put ones into data\n",
    "df[\"ones\"] = 1\n",
    "\n",
    "# drop outliers quick and dirty\n",
    "df = df.loc[df[\"gdp_pc\"] < 80,:]\n",
    "\n",
    "# maybe logs?\n",
    "df[\"ln_gdp_pc\"] = np.log(df[\"gdp_pc\"])\n",
    "df[\"ln_edgar_co2pc\"] = np.log(df[\"edgar_co2pc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52982298",
   "metadata": {},
   "source": [
    "## Exercise 1: Function for OLS coefficients <a name=\"Exercise-1:-Function-for-OLS-coefficients\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5ce629",
   "metadata": {},
   "source": [
    "- First, use the function `sm.OLS.from_formula` to regress `ln_edgar_co2pc` on `ln_gdp_pc`, save the results in an object called `ols_canned_results`, save the table with the regression results in an object called `ols_canned_results_table`, and display the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f6e1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df884a60",
   "metadata": {},
   "source": [
    "- OK so the canned routine gives us the OLS estiamates for $\\beta_0$ and $\\beta_1$, plus a bunch of other related statistics. That's convenient, though we'd like to understand what is behind all these estimates and numbers, wouldn't we? Of couuuurse. So let's use our knowledge of Python, our knowledge of the Python application of the sample-mean estimator and related statistics seen in the last class, and our new knowledge of the OLS equations to figure it out.\n",
    "- For the exercises of last class, we have built some functions for the **sample-mean estimator** using matrix notation. The key formula is $\\hat{\\beta}_{SM} = (\\boldsymbol{x}'\\boldsymbol{x})^{-1}(\\boldsymbol{x}'\\boldsymbol{y})$, which in Python is written as `betahat_SM = (inv(xdata.T @ xdata)) @ (xdata.T @ ydata)`.\n",
    "- Here is what we used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd25e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to transform panda series into vectors / matrices\n",
    "def data_to_matrix(data, variab_name):\n",
    "    \n",
    "    \"\"\" My Data to Matrix Function \"\"\"\n",
    "    \n",
    "    # store in matrixes\n",
    "    matrix = data.loc[:,variab_name].to_numpy()\n",
    "    \n",
    "    # make column vectors for arrays with less than 2 dimensions\n",
    "    if len(matrix.shape) == 1:\n",
    "        matrix = np.atleast_2d(matrix).T\n",
    "        \n",
    "    # return result\n",
    "    return matrix\n",
    "\n",
    "# define sample mean function\n",
    "def sample_mean_estimator(data, y, x):\n",
    "    \n",
    "    \"\"\" My Sample Mean Function \"\"\"\n",
    "    \n",
    "    # store in matrixes\n",
    "    ydata = data_to_matrix(data, variab_name = y)\n",
    "    xdata = data_to_matrix(data, variab_name = x)\n",
    "\n",
    "    # get sample mean\n",
    "    beta_hat_SM = (inv(xdata.T @ xdata)) @ (xdata.T @ ydata)\n",
    "\n",
    "    # return\n",
    "    return float(beta_hat_SM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8793a6ba",
   "metadata": {},
   "source": [
    "- Test these functions to compute the sample mean of `edgar_co2pc` and `gdp_pc`, and print the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b95a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7de95b",
   "metadata": {},
   "source": [
    "- Why is the sample-mean estimate for CO2 emissions per capita different from 5.02 (the one we computed during last class with the same QoG variable)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f480e",
   "metadata": {},
   "source": [
    "- Your answer here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d730f34",
   "metadata": {},
   "source": [
    "- Let's now write a function for the **OLS estimator**. As we have seen, the OLS is simply a generalization of the sample-mean estimator, as we move from a data vector $x$ with only ones to a data matrix $X$ with ones and the realizations of a random variable (GDP per capita). \n",
    "- Since we have been super good and we wrote the function for the sample-mean estimator already in matrix form and super generalized, we don't have to change much for the one of the OLS estimator. \n",
    "- Starting from the function `sample_mean_estimator`, write a function called `OLS_estimator_simple` to estimate the OLS coefficients. Tips:\n",
    "    - Instead of the argument `x`, lets follow the notation for matrixes and put `X`\n",
    "    - Instead of naming the output `beta_hat_SM`, name it `beta_hat_OLS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1acf6ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc4157",
   "metadata": {},
   "source": [
    "- Test the function by estimating the coefficients of the regression of `ln_edgar_co2pc` on `ln_gdp_pc`. Tips:\n",
    "    - The arguments of your function should take up these 2 variable names\n",
    "    - Remember that you need to estimate both $\\beta_0$ and $\\beta_1$ (remember the inputs you gave to the function for the sample mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13cbf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce8e7e8",
   "metadata": {},
   "source": [
    "- Are these coefficients the same ones you have obtained with the canned routine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c28363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aca6975",
   "metadata": {},
   "source": [
    "## Exercise 2: Make a nice output table <a name=\"Exercise-2:-Make-a-nice-output-table\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ca7fc",
   "metadata": {},
   "source": [
    "- OK but your results don't look nearly as cool as the ones of the canned method, which uses a `SimpleTable` to store them and display them. We don't know SimpleTables, but we do know panda dataframes! We could try to get a similar output by storing our results in a panda dataframe. To do that, we'll add a little chunk of code at the end of our `OLS_estimator_simple` function. \n",
    "- Write a new function called `OLS_estimator` by adding 2 chunks of code to the function `OLS_estimator_simple`:\n",
    "    1. Chunk that stores (i) the number of observations, (ii) the number of parameters, (iii) the degrees of freedom. Tip: this chunk will be between the chunks `store in matrixes` and `get OLS estimate`.\n",
    "    2. Chunk that stores the results in a dataframe and gives it back to us, in the form of the picture below (note that coefficient estimates are rounded to 4 decimals). Tip: this chunk will be after the chunk `get OLS estimate`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2304f6c5",
   "metadata": {},
   "source": [
    "<img src=\"https://i.ibb.co/wyt7tpz/Screen-Shot-2023-10-24-at-15-34-15.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2283f9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca5987",
   "metadata": {},
   "source": [
    "- Test the function by regressing `ln_edgar_co2pc` on a constant and `ln_gdp_pc` and display the regression coefficients. In addition, re-plot the canned results to make sure your numbers match with the numbers in the first column of the canned routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d700e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c04dac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee4b06b",
   "metadata": {},
   "source": [
    "## Exercise 3: Add statistics to the output table <a name=\"Exercise-3:-Add-statistics-to-the-output-table\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdc8b3d",
   "metadata": {},
   "source": [
    "- Now, you must have noticed that your table is a little smaller than the canned routine's, as you are missing all the nice statistics for statistical inference. Let's add them shall we? We can start with the estimates for the standard errors of the OLS coefficient estimators. \n",
    "- As we have seen above, the variance-covariance matrix of the OLS estimator is $\\sigma^2(\\boldsymbol{X}'\\boldsymbol{X})^{-1}$, and its estimator is $\\hat{\\sigma}^2(\\boldsymbol{X}'\\boldsymbol{X})^{-1}$. The estimators for the standard errors are the square roots of the diagonal elements of the estimator for variance-covariance matrix, i.e. for $\\hat{\\beta}_0$ is $\\sqrt{\\hat{\\sigma}^2_{OLS}S^{11}}$ and for $\\hat{\\beta}_1$ is $\\sqrt{\\hat{\\sigma}^2_{OLS}S^{22}}$.\n",
    "- Add a chunk of code to your function `OLS_estimator` to compute the estimates for the standard errors of the OLS estimates and store them in an added column of the output table titled `std err`. Tips:\n",
    "    - The chunk should be between `get OLS estimate` and `get table`, and the chunk `get table` should also be updated.\n",
    "    - In Python you can write $\\sigma^2(\\boldsymbol{X}'\\boldsymbol{X})^{-1}$ as `beta_hat_OLS_vcov = sigma2_hat_OLS * inv(xdata.T @ xdata)` (obtained with the OLS residuals and the estimator of the model's variance), and you can create the vector of estimates of the standard errors as `se_hat_OLS = np.atleast_2d(np.sqrt(betahat_OLS_vcov.diagonal())).T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1828a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a26b7dc",
   "metadata": {},
   "source": [
    "- Test the new version of the function and compare the results with the canned routine (all numbers should match):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8711c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "661c1160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef4e973",
   "metadata": {},
   "source": [
    "- Finally, update your function `OLS_estimator` to also compute test statistics, p-values and confidence intervals for your OLS estimates and add them to your output table in 4 new columns. The function should give an output table that is very similar to the one of the canned method. Tips:\n",
    "    - You can leverage your knowledge of functions for (i) test statistics, (ii) p-values and (iii) confidence intervals for the sample-mean estimates and apply it for OLS. The only difference is that now you need to obtain them for 2 coefficients, rather than only one. So you'll have to create a loop. \n",
    "    - Also, in the exercises of last class on the sample mean, we did a large-sample version of the t-statistic, in which we assumed it distributed like a normal. As the canned routine does not make this assumption and uses the t-student distribution, with related degrees-of-freedom correction, let's also do it for our function. In the exercises of 2 classes ago, we have seen this test for the sample mean estimator (it was called one-sample t-test for the sample mean). Here you need to do the same thing, just for the OLS estimator. Remember that, as we are using a t-student distribution, the Python function that gets you the area underneath the cumulative density function for a given t-statistic value is `stats.t.cdf`. Also, the Python function that gets you a critical value from a given critical percentage (the inverse of `stats.t.cdf`) is `stats.t.ppf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a407ee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90c201b",
   "metadata": {},
   "source": [
    "- Test the updated version of the function `OLS_estimator` and compare the results with the canned routine (all numbers should match):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20b6c36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "963ab623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
