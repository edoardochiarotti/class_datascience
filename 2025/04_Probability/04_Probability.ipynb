{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69c731d5",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/edoardochiarotti/class_datascience/blob/main/2024/04_Probability/04_Probability.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f331f3b2-fc36-46ed-96a8-66897b24749a",
   "metadata": {},
   "source": [
    "# Probability and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0326aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGES\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random as rd\n",
    "import statistics as st\n",
    "import pandas as pd\n",
    "\n",
    "# SEABORN THEME\n",
    "scale = 0.4\n",
    "W = 16*scale\n",
    "H = 9*scale\n",
    "sns.set(rc = {'figure.figsize':(W,H)})\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9db29f-29a3-4439-9c46-25618dc1fa86",
   "metadata": {},
   "source": [
    "Main References:\n",
    "- A great source to learn probability and Statistics with Python [is this website](https://ethanweed.github.io/pythonbook/landingpage.html) by Weed and Navarro (translation of Navarro’s book [Learning Statistics with R](https://learningstatisticswithr.com/) in Python). For this Notebook, we borrow from Weed and Navarro's chapters on Statistical Theory. \n",
    "- For more theory in statistics and econometrics, we rely on\n",
    "    - J. Wooldridge, Econometric Analysis of Cross Section and Panel Data, MIT Press, 2002\n",
    "    - William H. Greene, Econometric Analysis, sixth edition, Pearson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b958ce-d6a4-4ff3-80ff-66855cf271b4",
   "metadata": {},
   "source": [
    "## Content\n",
    "- [Concepts of Probability and Statistics](#Concepts-of-Probability-and-Statistics)\n",
    "    - [Random Variables and Probability Distributions](#Random-Variables-and-Probability-Distributions)\n",
    "    - [Discrete Random Variables and Mass Functions](#Discrete-Random-Variables-and-Mass-Functions)\n",
    "    - [Continuous Random Variables and Density Functions](#Continuous-Random-Variables-and-Density-Functions)\n",
    "    - [Moments of Density Functions](#Moments-of-Density-Functions)\n",
    "    - [The Univariate Normal Distribution](#The-Univariate-Normal-Distribution)\n",
    "    - [The Normal Distribution in Python](#The-Normal-Distribution-in-Python)\n",
    "    - [Population, Random Sample and Sample](\"Population,-Random-Sample-and-Sample\")\n",
    "    - [Estimators](#Estimators)\n",
    "    - [Confidence Intervals](#Confidence-Intervals)\n",
    "    - [Hypothesis Testing](#Hypothesis-Testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c790e33-1c97-4ec4-856b-1a9d8a8f06bb",
   "metadata": {},
   "source": [
    "## Concepts of Probability and Statistics <a name=\"Concepts-of-Probability-and-Statistics\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35a553d-0332-4ff2-9c05-bfd84d3a50f2",
   "metadata": {},
   "source": [
    "### Random Variables and Probability Distributions <a name=\"Random-Variables-and-Probability-Distributions\"></a>\n",
    "- A **random variable** is a variable whose possible values are numerical outcomes of a random phenomenon, described by a probability distribution (for a more formal definition, see the [Wikipedia, Random variable](https://en.wikipedia.org/wiki/Random_variable)).\n",
    "- In probability theory and statistics, a **probability distribution** is the mathematical function that gives the probabilities of occurrence of different possible outcomes ([Wikipedia, Probability distribution](https://en.wikipedia.org/wiki/Probability_distribution))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8abb98-4dec-46e8-994c-65a489371931",
   "metadata": {},
   "source": [
    "### Discrete Random Variables and Mass Functions <a name=\"Discrete-Random-Variables-and-Mass-Functions\"></a>\n",
    "- If the range of the random variable is countable, the random variable is called a **discrete random variable** and its distribution is a discrete probability distribution, i.e. can be described by a probability mass function that assigns a probability to each value in the range of the random variable ([Wikipedia, Random variable](https://en.wikipedia.org/wiki/Random_variable))\n",
    "- The **probability mass function** (PMF) can be represented like this:\n",
    "\n",
    "\n",
    "<img src=\"https://i.ibb.co/xJDCQJJ/Screen-Shot-2023-10-10-at-19-50-23.png\" width=\"300\">\n",
    "\n",
    "Source: Wikipedia, Probability Mass Function\n",
    "\n",
    "- It's formal definition is a function f : R -> [0,1] defined by:\n",
    "\n",
    "<br>\n",
    "$$\n",
    "f(y)=P(y)\n",
    "$$\n",
    "<br>\n",
    "\n",
    "for $-\\infty < 0 < \\infty$, where P is a probability measure. The probabilities associated with all (hypothetical) values must be non-negative and sum up to 1 ([Wikipedia, Probability Mass Function](https://en.wikipedia.org/wiki/Probability_mass_function))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394d53f0",
   "metadata": {},
   "source": [
    "Example: **Bernoulli Distribution**\n",
    "- You should always go on Wikipedia and check the page for the distribution, here is the one for the [Bernoulli Distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution)\n",
    "- PMF: \n",
    "$ \n",
    "f(y;p) =\n",
    "      \\begin{cases}\n",
    "        p     & \\text{if $y = 1$}, \\\\\n",
    "        1 - p & \\text{if $y = 0$}.\n",
    "      \\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bff1985",
   "metadata": {},
   "source": [
    "- Representation of PMF for different p:\n",
    "\n",
    "<img src=\"https://i.ibb.co/W54kCDM/Screen-Shot-2023-10-10-at-19-56-38.png\" width=\"300\">\n",
    "\n",
    "Source: Wikipedia, Bernoulli Distribution\n",
    "- A fair coin toss follows a Bernoulli with p = 0.5. \n",
    "- Note that the Bernoulli Distribution is just a special case of a **Discrete Binomial Distribution** with one repetition. You should go check the Wikipedia page of a [Discrete Binomial Distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution). For example, the Binomial Distribution tells you what is the probability of getting heads if we flip a coin n times, or what is the probability of getting 6 if we roll a dice 3 times.\n",
    "- For the sake of time, here we do not simulate these distributions. You can check [this article by DataCamp](https://www.datacamp.com/tutorial/probability-distributions-python) and the chapter [Introduction to Probability](https://ethanweed.github.io/pythonbook/04.02-probability.html#) by Ethan Weed to see how to simulate a Bernoulli and a Discrete Binomial in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1bb22d",
   "metadata": {},
   "source": [
    "### Continuous Random Variables and Density Functions <a name=\"Continuous-Random-Variables-and-Density-Functions\"></a>\n",
    "- If the range of the random variable is uncountably infinite (usually an interval) then the random variable is called a **continuous random variable**. \n",
    "- **Example**: suppose bacteria of a certain species typically live 4 to 6 hours. The probability that a bacterium lives exactly 5 hours is equal to zero. A lot of bacteria live for approximately 5 hours, but there is no chance that any given bacterium dies at exactly 5.00 hours. However, the probability that the bacterium dies between 5 hours and 5.01 hours is quantifiable ([Wikipedia, Probability density function](https://en.wikipedia.org/wiki/Probability_density_function)).\n",
    "- The probability distribution of a continuous random variable can be described by a **probability density function** (PDF), which assigns probabilities to intervals. In particular, **each individual point must necessarily have probability zero for a (absolutely) continuous random variable** ([Wikipedia, Random Variable](https://en.wikipedia.org/wiki/Random_variable)).\n",
    "- More formally, the density function $f(y)$ of a continuous random variable $y$ is the non-negative (Lebesgue-integrable) function that satisfies the following condition:\n",
    "    - $P(a<y<b)=\\int_a^bf(y)dy$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f71c9",
   "metadata": {},
   "source": [
    "### Moments of Density Functions <a name=\"Moments-of-Density-Functions\"></a>\n",
    "- The shape of density functions depends on quantitative measures that are called **\"moments\"**. In mathematics, the moments of a function are quantitative measures related to the shape of the function's graph. If the function is a probability distribution, then the first moment is the expected value, the second central moment is the variance, the third standardized moment is the skewness, and the fourth standardized moment is the kurtosis ([Wikipedia, Moments](https://en.wikipedia.org/wiki/Moment_(mathematics))).\n",
    "- The general formula for the **n-th moment** of a real-valued continuous function $f(y)$ of a real variable about a value c is the following:\n",
    "<br>\n",
    "<br>\n",
    "$$m_n=\\int_{-\\infty}^{+\\infty}(y-c)^nf(y)dy$$\n",
    "<br>\n",
    "- The value c is the previous moment on which the moment in question depends. For the expected value, which is the first moment, we'll have $c=0$, as the mean does not depend on any previous moment. For the variance, which is the second moment, we'll have $c=\\beta$.\n",
    "- Let's define the **expected value**, i.e. the first moment. Intuitively, the expected value of a discrete random variable is a weighted average of all possible outcomes, where the weights are the probabilities that these outcomes materialize (given by the probability mass function). In the case of a continuum random variable, the expectation is defined by integration ([Wikipedia, Expected Value](https://en.wikipedia.org/wiki/Expected_value#Random_variables_with_density)). The notation of the expected value for a random variable $y$ is $E(y)$. The general formula for a continuous random variable is:\n",
    "<br>\n",
    "<br>\n",
    "$$E[y]=\\int_{-\\infty}^{\\infty}yf(y)dy$$\n",
    "<br>\n",
    "- For the properties of the expected value, i.e. non-negativity, linearity of expectations, monotonicity, etc, you can check the page [Wikipedia, Expected Value - Properties](https://en.wikipedia.org/wiki/Expected_value#Properties).\n",
    "- Let's now define the **variance**, i.e. second moment. The variance is the expectation of the squared deviation of a random variable from its first moment ([Wikipedia, Variance](https://en.wikipedia.org/wiki/Variance)). Intuitively, the variance is a measure of dispersion, meaning it is a measure of how far a set of numbers is spread out from their average value. The notation for the variance of a random variable $y$ is $V(y)$. The general formula for the variance of a continuous random variable is:\n",
    "<br>\n",
    "<br>\n",
    "$$V[y]=\\int_{-\\infty}^{\\infty}(y-E(y))^2f(y)dy$$\n",
    "<br>\n",
    "- Given it's definition, the variable can be re-written in terms of the expected value as follows:\n",
    "<br>\n",
    "<br>\n",
    "$$V[y]=E[(y-E[y])^2]=E[y^2]-E[y]^2$$\n",
    "<br>\n",
    "- For the properties of the variance you can check the page [Wikipedia, Variance - Properties](https://en.wikipedia.org/wiki/Variance#Properties).\n",
    "- Famous probability distributions of continuous random variables that are used often in statistics are the [Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution), [Chi-squared Distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution), [Student's t-distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution) and [F-distribution](https://en.wikipedia.org/wiki/F-distribution). All of them can be described by a PDF. In what follows we'll consider the **Normal Distribution**, its PDF and its moments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe7272a",
   "metadata": {},
   "source": [
    "### The Univariate Normal Distribution <a name=\"The-Univariate-Normal-Distribution\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0647c811",
   "metadata": {},
   "source": [
    "- The **univariate normal distribution**, or more simply normal distribution, or Gaussian distribution, or Bell curve, is a type of continuous probability distribution for one real-valued random variable.\n",
    "- Here we refer to the normal distribution as the univariate normal distribution. It is **univariate** because it refers to one random variable. If we had more random variables, we would have a joint, or multivariate, normal distribution (see later).\n",
    "- As we have mentioned, a random variable is a variable whose possible values are numerical outcomes of a random phenomenon described by a probability distribution. Let's say that this probability distribution is a normal. This means that the probability that a random variable takes certain values is described by a normal distribution. A shorter way to say the same thing is to say **\"the random variable is normally distributed\"**, or \"the random variable follows a normal distribution\". \n",
    "- As usual, you should check the Wikipedia page of the [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution), in which you can see the the density function and its parameters / moments.\n",
    "- The probability density function (**PDF**) that describes the (univariate) normal probability distribution of a random variable is the following:\n",
    "<br><br>\n",
    "$$f(y)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{y-\\beta}{\\sigma}\\right)^{2}\\right)$$\n",
    "<br>\n",
    "- $y$ are the values that the random variable takes. You can see that the values of this function depend on the constant $\\pi$ (fixed) and parameters $\\beta$ and $\\sigma$.\n",
    "- We can apply the formula for the expected value to obtain its **expected value**:\n",
    "<br><br>\n",
    "$$E(y)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{+\\infty}y\\exp\\left(-\\frac{1}{2}\\left(\\frac{y-\\beta}{\\sigma}\\right)^{2}\\right)dy = \\beta$$\n",
    "<br>\n",
    "- So the expected value of a normal density equals the parameter $\\beta$. We can do the same for the second moment, i.e. the **variance**:\n",
    "<br><br>\n",
    "$$V(y)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{+\\infty} (y-\\beta)^2 \\exp\\left(-\\frac{1}{2}\\left(\\frac{y-\\beta}{\\sigma}\\right)^{2}\\right)dy = \\sigma^2$$\n",
    "<br>\n",
    "- So to **recap**, we start by the density function of a normally distributed variable $y$, and this function has some parameters $\\beta$ and $\\sigma^2$ inside. By applying formulas to this function for the moments, we can see that the expected value (first moment) is actually the parameter $\\beta$, and the variance (second moment) is actually the parameter $\\sigma^2$. So we can say that our random variable $y$ is normally distributed with mean $\\beta$ and variance $\\sigma^2$, which in notation is written as $y \\sim \\mathcal{N}(\\beta,\\,\\sigma^{2})$.\n",
    "- A special case of the normal distribution is called **standard normal distribution**, with the following density function, moments and notation:\n",
    "<br><br>\n",
    "$$f(y)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{y^2}{2}\\right)$$\n",
    "<br>\n",
    "$$E(y)=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{+\\infty}x\\exp\\left(-\\frac{y^2}{2}\\right)dy = 0$$\n",
    "<br>\n",
    "$$V(y)=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{+\\infty} x^2 \\exp\\left(-\\frac{y^2}{2}\\right)dy = 1$$\n",
    "<br>\n",
    "$$y \\sim \\mathcal{N}(0,\\,1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b233fa",
   "metadata": {},
   "source": [
    "### The Normal Distribution in Python <a name=\"The-Normal-Distribution-in-Python\"></a>\n",
    "- Ok great! Let's now see what all of this \"really\" means with the help of Python. \n",
    "- To work with random variables and distributions in **Python**, we use a combination of our beloved packages `numpy` and `scipy`. To get the values $f(x)$ of the probability density function of a normal distribution with $\\beta=0$ and $\\sigma^2=1$ (standard normal distribution), we use some `numpy` functions and the function `stats.norm.pdf` from `scipy` (we imported scipy.stats as stats):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7393a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set mean, variance and standard deviation\n",
    "beta = 0\n",
    "sigma2 = 1\n",
    "sigma = np.sqrt(sigma2)\n",
    "\n",
    "# get range of the PDF (3 standard deviations away from the mean)\n",
    "pdf_range = np.linspace(beta - 3*sigma, beta + 3*sigma, 100) \n",
    "\n",
    "# return densities of the PDF\n",
    "pdf_dens = stats.norm.pdf(pdf_range, beta, sigma)\n",
    "pdf_dens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae6fdd9",
   "metadata": {},
   "source": [
    "To plot these values, i.e. to plot the PDF, let's use the `seaborn` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df18e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot, set labels, and remove top and right spines\n",
    "fig = sns.lineplot(x = pdf_range, y = pdf_dens)\n",
    "plt.xlabel('Observed Value')\n",
    "plt.ylabel('Probability Density')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6291a974",
   "metadata": {},
   "source": [
    "- As mentioned, the values of the function are as such that the area below the function in a given interval represents the probability that the random variable takes values within that interval. \n",
    "- **ONCE AGAIN (yeah one more time): the values of the density functions are not probabilities! The probability that a continous random variable takes a specific value is zero! The values of the density functions are values such that the underling area of the function equals the probability that the random variable take values in a specific interval**.\n",
    "- In the case of the normal distribution, the probability that the random variable takes values in the interval of +/- one standard deviation from the mean is **68.3%**. Furthermore, the probability that the random variable takes values in the interval of +/- two standard deviations from the mean is **95.4%**.\n",
    "- This is shown in the figures below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c36f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set figure parameters\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5), sharey=False)\n",
    "\n",
    "# plot densities\n",
    "sns.lineplot(x = pdf_range, y = pdf_dens, ax=axes[0])\n",
    "sns.lineplot(x = pdf_range, y = pdf_dens, ax=axes[1])\n",
    "\n",
    "# add shaded area\n",
    "x_fill1 = np.arange(-1*sigma, 1*sigma, 0.001)\n",
    "x_fill2 = np.arange(-2*sigma, 2*sigma, 0.001)\n",
    "y_fill1 = stats.norm.pdf(x_fill1, beta, sigma)\n",
    "y_fill2 = stats.norm.pdf(x_fill2, beta, sigma)\n",
    "axes[0].fill_between(x_fill1, y_fill1, 0, alpha=0.2, color='blue')\n",
    "axes[1].fill_between(x_fill2, y_fill2, 0, alpha=0.2, color='blue')\n",
    "\n",
    "# add axis titles\n",
    "axes[0].set_title(\"Shaded Area = 68.3%\")\n",
    "axes[1].set_title(\"Shaded Area = 95.4%\")\n",
    "axes[0].set(xlabel='Observed value', ylabel='Probability density')\n",
    "axes[1].set(xlabel='Observed value', ylabel='Probability density')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5543e497",
   "metadata": {},
   "source": [
    "- Finally, the probability that the random variable takes values in the interval of +/- three standard deviations from the mean is 99.7% (not shown). Note that **these thresholds always hold** for a normal distribution, regardless of the values of $\\beta$ and $\\sigma^2$.\n",
    "- Indeed, the normal distribution can have different shapes, depending on the values we set for $\\beta$ and $\\sigma^2$. The **shape of the curves** above is as such because we have set the mean $\\beta=0$ and the variance $\\sigma^2=1$. If we change these parameters, the shape of this curve will change. For example, if we increase the variance $\\sigma^2$ from 1 to 2, the curve will be \"flatter\", which means that the values that the random variable takes will be more spread, i.e. farer away from the mean (\"fat tails\"). \n",
    "- The figure below shows the density of the standard normal distribution done above with $\\beta=0$ and $\\sigma^2=1$ (blue) and the density of the normal distribution with $\\beta=0$ and $\\sigma^2=2$ (red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4032c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get second curve\n",
    "sigma2_bis = 2\n",
    "sigma_bis = np.sqrt(sigma2_bis)\n",
    "pdf_range_bis = np.linspace(beta - 3*sigma_bis, beta + 3*sigma_bis, 100)\n",
    "pdf_dens_bis = stats.norm.pdf(pdf_range_bis, beta, sigma_bis)\n",
    "\n",
    "# plot densities\n",
    "sns.lineplot(x = pdf_range, y = pdf_dens)\n",
    "sns.lineplot(x = pdf_range_bis, y = pdf_dens_bis)\n",
    "\n",
    "# add shaded area\n",
    "x_fill1 = np.arange(-1, 1, 0.001)\n",
    "x_fill2 = np.arange(-1.41, 1.41, 0.001)\n",
    "y_fill1 = stats.norm.pdf(x_fill1, beta, sigma)\n",
    "y_fill2 = stats.norm.pdf(x_fill2, beta, sigma_bis)\n",
    "plt.fill_between(x_fill1, y_fill1, 0, alpha=0.1, color=\"blue\")\n",
    "plt.fill_between(x_fill2, y_fill2, 0, alpha=0.2, color = \"red\")\n",
    "\n",
    "# add axis titles\n",
    "plt.title(\"Shaded Area = 68.3%\")\n",
    "plt.xlabel('Observed Value')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.xlim((beta - 3*sigma), (beta + 3*sigma))\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2af849",
   "metadata": {},
   "source": [
    "- The surface of the shaded areas of the two curves is the same, i.e. they refer to 68.3% of the values. Remember that this area, i.e. the integral between -1 and 1 for the first curve and -2 and 2 for the second curve, is the probability that the respective random variable takes values between, respectively, -1 and 1 and -2 and 2. \n",
    "- So, while for the first random variable, 68.3% of the values are condensed around the mean (which is 0), for the second random variable 68.3% of the values are more spread around the mean (which is again 0). This example shows that the shape of the density function, and therefore the probability distribution, depends on its variance. \n",
    "- In general, the shape of the normal density function depends on its **first two moments**, i.e. the mean and the variance. For an example of how a change in the mean changes the density (it's a shift in this case), see [Introduction to Probability](https://ethanweed.github.io/pythonbook/04.02-probability.html#fig-normal) by Weed and Navarro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e814d0b",
   "metadata": {},
   "source": [
    "### Population, Random Sample and Sample <a name=\"Population,-Random-Sample-and-Sample\"></a>\n",
    "- Assume that you want to study the characteristics of the individuals of a population (individuals are not necessarily human beings, they can be firms, countries, sectors, ...). **Sampling** is the only way to do so if the population is large, as observing the entire population is typically impossible. For instance, we want to study the CO2 emission per capita of the population of swiss students. However, we do not have data for all the active students in Switzerland, so let's say we will have to work only with a sample of students (e.g. this class).\n",
    "- Statisticians operationalize the concept of a **“population”** in terms of mathematical objects that they know how to work with, i.e. probability distributions. In other words, we, novel statisticians, will **model** the population as a probability distribution.\n",
    "- For example, the distribution of CO2 emissions per capita of the active students in Switzerland can be represented by a normal distribution with mean, say, 5 tonnes per capita (or more generally $\\beta$) and variance, say, 2 tonnes per capita (or more generally $\\sigma^2$). In concise notation, this **population model** can be written as follows:\n",
    "<br><br>\n",
    "$$\n",
    "y \\sim \\mathcal{N}(\\beta,\\,\\sigma^2)\n",
    "$$ \n",
    "<br>\n",
    "- As said above, we will not be able to observe the full population of active students. We'll just be able to observe a sample of N students for which we have data. Before even asking you guys how much are your CO2 emissions and obtain our sample, we want to say something about the properties of such sample. Let's then refer to a hypothetical sample (not filled with data yet!) as the **random sample**:\n",
    "<br><br>\n",
    "$$\\{y_1,...,y_N\\}$$\n",
    "<br>\n",
    "- We assume that each observation of the random sample, $y_i$, is generated by an underlying process, called **Data Generating Process (DGP)**, described by the distribution of the population, i.e. a normal distribution with mean $\\beta$ and variance $\\sigma^2$. So each observation in the random sample is a random variable that follows the distribution of the population.\n",
    "- For example, let's think that we are about to draw our sample of students for which we have data on CO2 emissions per capita. We assume that there is a 95% chance that CO2 emissions per capita of the first hypothetical student in our sample (first observation) will be between 1 tonne ($5-2\\times2$) and 9 tonnes ($5+2\\times2$). In other words, we assume that the generating process of the data for the first hypothetical student follows the distribution of the population, i.e. a normal distribution with mean 5 (or more generally $\\beta$) and variance 2 (or more generally $\\sigma^2$). We do the same for the second hypothetical student (second observation in the random sample), the third hypothetical student (third observation in the random sample) ... and the same for the Nth hypothetical student (Nth observation in the random sample).\n",
    "- Formally, we can express this concept as the **model of the DGP**:\n",
    "<br><br>\n",
    "$$\n",
    "y_i \\sim \\mathcal{N}(\\beta,\\,\\sigma^2), \\, \\,  \\text{for }i=1,...,N\n",
    "$$ \n",
    "<br>\n",
    "- As $y_i$ are independent from each other and all follow the distribution of the population (normal), we say that these random variables are independent and identically distributed (i.i.d.). Weird right? Yeah, it'll become clearer in a moment when we do this with Python.\n",
    "- Now we know the properties of our random sample, which will be useful to describe estimators. As we are happy with all these properties, let's now think of the moment when we go out there, we get the data on CO2 emission per capita and we obtain our **sample**, which is just one realization of the random sample:\n",
    "<br><br>\n",
    "$$\\{y_1^s,...,y_N^s\\}$$ \n",
    "<br> \n",
    "- Our objective will be to draw conclusions, or *infer* something, about the entire population by relying only on this realization - in statistical language, we do **inference**.\n",
    "- OK let's follow the example of CO2 per capita mentioned above. We model the population of CO2 per capita as  a normal distribution with population mean $\\beta$ = 5 tonnes per capita and population variance $\\sigma^2 =$ 2 tonnes per capita.\n",
    "- Using the **Python** functions that we have learned before, let's draw the density function of the population, which is a normal density function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a10e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "beta = 5\n",
    "sigma = 2\n",
    "\n",
    "# get range and densities of the PDF\n",
    "pdf_range = np.linspace(beta - 3*sigma, beta + 3*sigma, 100) \n",
    "pdf_dens = stats.norm.pdf(pdf_range, beta, sigma)\n",
    "\n",
    "# Plot, set labels, and remove top and right spines\n",
    "fig = sns.lineplot(x = pdf_range, y = pdf_dens)\n",
    "plt.xlabel('Observed Value')\n",
    "plt.ylabel('Probability Density')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f62b25",
   "metadata": {},
   "source": [
    "- OK great! Now let's create a sample by drawing from this distribution of the population (which we know to be normal). By \"drawing a sample\" we mean that, for each individual, we'll draw a value of CO2 emission per capita from the distribution of the population, represented by the normal density. And let's say we want to end up with a sample for 100 individuals, so we'll repeat this draw 100 times.\n",
    "- To draw from a known density, we'll use the numpy function `np.random.normal`. This is a **random number generator**, which uses a built-in algorithm to generate random numbers from a normal density. If we want our results to be reproducible, we must set the first number to initialize the random number generator. Indeed, the random number generator needs a number to start with (a seed value), to be able to generate a random number. If we do not set it, the generator will start wherever it wants. To set this \"seed number\", we use the function `np.random.seed` from the package random, with the first number being 12345 (you can use the number you want for your own simulations, i.e. 10, 1, etc):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df2bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6362deb7",
   "metadata": {},
   "source": [
    "- Cool, let's now use `np.random.normal` and the for loop to do 1,000 draws of CO2 per capita from the normal distribution of the population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ea68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "N = 100\n",
    "\n",
    "# use a for loop to draw a score for 1000 individuals\n",
    "draws_co2_list = []\n",
    "for i in range(N):\n",
    "    x = float(np.random.normal(loc = beta, scale = sigma, size=1))\n",
    "    draws_co2_list.append(x)\n",
    "\n",
    "# display\n",
    "draws_co2_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3edefd7",
   "metadata": {},
   "source": [
    "- OK, nice. We have drawn one value of CO2 per capita at the time from the normal density that represents the distribution of the population, using the for loop. We have used the for loop to make it clear that it is one draw at the time. Now that it's clear, a faster way to do it is to use the argument `size = 100`, which will do exactly what we did with the for loop, just more efficiently. Let's use that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1013bc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "draws_co2 = np.random.normal(loc = beta, scale = sigma, size=N)\n",
    "draws_co2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d45eb0",
   "metadata": {},
   "source": [
    "- OK we got a numpy.array as an output, even better. Now, are the items of the first list we saved the same of the items of the numpy.array we saved? Absolutely not. Why? Because the random number generator will generate different random numbers every time we \"launch it\". What we did by setting the seed is just making sure that when we re-run the seed() command, the first list will be the same of the first list we have run, and the numpy.array will be the same of the numpy.array we have run. If you did not understand this concept, re run the cells from seed() onwards, and check it out. Indeed, **reproducibility**.\n",
    "- Let's now plot these emissions per capita by using a **histogram**, done with `seaborn.histplot()`. You should absolutely read the documentation of this function [here](https://seaborn.pydata.org/generated/seaborn.histplot.html). As mentioned in the documentation, a histogram is a classic visualization tool that represents the distribution of one or more variables by counting the number of observations that fall within discrete bins. The key words here are **counting** and **bins**. The width of a bin represents the cutoff values that group the observations within that bin, and the height of the bean represents the number of observations (count) that fall within that bin. Let's try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d7388",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(draws_co2)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b8115",
   "metadata": {},
   "source": [
    "- Before commenting on the shape of this thing (looks familiar?), let's understand what we are doing. The default value of the argument bin is `bin='auto'`. That means that histoplot picks some beans and applies them to the data to get the histogram. OK but what if I wanted to know which bins it is using? We can do it with `np.histogram_bin_edges()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612b89fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.histogram_bin_edges(draws_co2, bins='auto')\n",
    "bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec6534d",
   "metadata": {},
   "source": [
    "- This is very important, as it shows which bins we are using to group our observations. Do they make sense or not? Maybe, or maybe not, as we might have wanted cutting points at say 3, 3.2, 3.4, and so on. We'll leave the decision on which **cutting points** to use to another day. For the moment it is good that you are aware that these cutting points can largely change your graph, and therefore you should always look for where they are if they are set automatically.\n",
    "- Back to the shape of the histogram. Does it look familiar? Of course it does, it looks like a normal density. Indeed, the beans around the mean contain the highest number of observations and are taller than the beans far away from the mean. For example, there are just about 5 individuals with emissions per capita above 8 tonnes. This is happening because we drew random observations from a normal distribution. The way these random observations are distributed, which is represented by the histogram, approximates a normal distribution.\n",
    "- Let's check how good is the approximation by plotting the **density function** on the top of the histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2f074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram\n",
    "sns.histplot(draws_co2)\n",
    "\n",
    "# plot density\n",
    "sns.lineplot(x = pdf_range, y = pdf_dens)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2e5d0a",
   "metadata": {},
   "source": [
    "- Uh! We do not see the plot of the density. You should know why. If you don't, you are not understanding what the histogram is showing you, and what the density function is, so you should go back and read the functions' documentation. And the answer is not using the kde argument, which is highly misleading in this case.\n",
    "- ... Or you can just keep reading here (though you should really read the documentation). We are not seeing the density because on the y axis we are plotting the count of observations in each bin. The densities values are between 0.00 and 0.40, as seen above, so when we plot it in a graph with y between 0 and 30, it will be shrunk at the bottom of our graph. What we want is to standardize the bins so that their height, instead of giving us the count, gives us the density values that are related to that bin. We can do so with the argument `stat=\"density\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram\n",
    "sns.histplot(draws_co2, stat = \"density\")\n",
    "\n",
    "# plot density\n",
    "sns.lineplot(x = pdf_range, y = pdf_dens)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47408ebb",
   "metadata": {},
   "source": [
    "- Woah! OK we can see that the distribution of our sample of CO2 emissions per capita, represented by the histogram, approximates kinda well the distribution of our population of CO2 emissions, represented by the density.\n",
    "- It's \"kinda well\" because the number of individuals in our sample (draws) is not very high, i.e. 100. If we increase the number of individuals (draws) to let's say 100,000, then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b565b147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100,000 draws\n",
    "N = 100000\n",
    "draws_co2 = np.random.normal(loc = beta, scale = sigma, size = N)\n",
    "\n",
    "# plot histogram\n",
    "sns.histplot(draws_co2, stat = \"density\")\n",
    "\n",
    "# plot density\n",
    "sns.lineplot(x = pdf_range, y = pdf_dens)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1734136",
   "metadata": {},
   "source": [
    "- Re-woah! The distribution of our sample (histogram) approximates better the distribution of the population (density). So the moments of the sample become more similar to the moments of the distribution, which is basically the law of large numbers.\n",
    "- **In real life**, we will not be drawing from a distribution. What we'll have is a sample of data and what we'll do is assuming that this sample was randomly drawn from the distribution of the population. As we do that, we can use estimators that produce estimates of the moments of the population. Next we introduce estimators by considering the most simple one, the sample mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dcea50",
   "metadata": {},
   "source": [
    "### Estimators <a name=\"Estimators\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12deb0cf",
   "metadata": {},
   "source": [
    "- In the CO2-emission example we have seen, we actually knew the population parameters ahead of time. What if we don't know them, which is what usually happens in real life? Well, we have to estimate it using an estimator.\n",
    "- An **estimator** is any function $T(y_1, y_2, ...y_N)$ of a random sample, and is itself a random variable, which means it has a probability distribution, which we call the **sampling distribution**. This distribution is characterized by moments such as the expectation $E(.)$, the variance $V(.)$ or higher moments. \n",
    "- We call a **point estimate**, or **estimate**, the realized value of an estimator (i.e. a number) obtained when a sample is actually taken.\n",
    "- Any statistic is an estimator. For instance, assume that you have a random sample $(y_1, y_2, ...y_N)$ of i.i.d. random variables which follow Normal distributions with mean $\\beta$ and variance $\\sigma^2$. The **sample mean** is a \"good\" (efficient and unbiased) estimator of $\\beta$:\n",
    "<br><br>\n",
    "$$\\hat{\\beta}=\\frac{1}{N}\\sum_{i=1}^{N}y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bd21c7",
   "metadata": {},
   "source": [
    "- Before analysing the sampling distribution of this estimator, one quick word on the **Law of Large Numbers**: as the sample size increases, estimates converge to population moments. The law of large numbers is the thing we can use to justify our belief that collecting more and more data will eventually lead us to the truth.\n",
    "- So it is obvious: the more data we have the better it is. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34530cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 10\n",
    "N2 = 100\n",
    "N3 = 10000\n",
    "\n",
    "co2_10 = np.random.normal(loc = beta, scale = sigma, size = N1)\n",
    "co2_100 = np.random.normal(loc = beta, scale = sigma, size = N2)\n",
    "co2_10000 = np.random.normal(loc = beta, scale = sigma, size = N3)\n",
    "\n",
    "print(\"10 samples. Mean: \", st.mean(co2_10))\n",
    "print(\"100 samples. Mean: \", st.mean(co2_100))\n",
    "print(\"10000 samples. Mean: \", st.mean(co2_10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d384a0",
   "metadata": {},
   "source": [
    "- Indeed, the mean gets more precise as $N$ goes up.\n",
    "- The law of large numbers is a very powerful tool, but it’s not going to be good enough to answer all our questions. Among other things, all it gives us is a “long run guarantee”. In the long run, if we were somehow able to collect an infinite amount of data, then the law of large numbers guarantees that our sample statistics will be correct.\n",
    "- Knowing that an infinitely large data set will tell me the exact value of the population mean is cold comfort when my actual data set has a sample size of $N=100$. In real life, then, we must know something about the behaviour of the sample mean when it is calculated from a more modest data set!\n",
    "- OK let's now explore the **sampling distribution of the sample mean**. I can approximate the sampling distribution by repeateadly estimating the sample mean x times on x different generated samples. Let's do it with a sample size of 5 observations of our CO2 emissions per capita. We will compare the resulting sampling distribution with the density of the population, as we want to see if the mean of the sampling distribution is close to the mean of the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e5f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 10000 simulated experiments with 1 subject each, and calculate the sample mean for each experiment\n",
    "sample_size = 5\n",
    "beta_hats = []\n",
    "for i in range(1,10000):\n",
    "    beta_hat = st.mean(np.random.normal(loc = beta, scale = sigma, size = sample_size).astype(int))\n",
    "    beta_hats.append(beta_hat)\n",
    "\n",
    "\n",
    "# plot a histogram of the distribution of sample means, together with the population distribution\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(beta_hats, ax = ax, binwidth=1)\n",
    "ax2 = ax.twinx()\n",
    "sns.lineplot(x = pdf_range, y = pdf_dens, ax = ax2, color = 'black')\n",
    "\n",
    "print(st.mean(beta_hats))\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a21bf56",
   "metadata": {},
   "source": [
    "- OK so it seems that the mean of the sampling distribution is quite close to the mean of the population.\n",
    "- Another thing that this graph is telling us is that the experiment with 5 individuals not very accurate. If we repeat the experiment, the sampling distribution tells us that we can expect to see a sample mean anywhere between 2 and 7 tonnes. So yes, our sample is bad.\n",
    "- Let's now study what happens when we move our sample size. Let's start with sample size euqals 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bd29e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 10000 simulated experiments with 1 subject each, and calculate the sample mean for each experiment\n",
    "sample_size = 1\n",
    "beta_hats = []\n",
    "for i in range(1,10000):\n",
    "    beta_hat = st.mean(np.random.normal(loc = beta, scale = sigma, size = sample_size).astype(int))\n",
    "    beta_hats.append(beta_hat)\n",
    "\n",
    "\n",
    "# plot a histogram of the distribution of sample means, together with the population distribution\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(beta_hats, ax = ax, binwidth=1)\n",
    "ax2 = ax.twinx()\n",
    "sns.lineplot(x = pdf_range, y = pdf_dens, ax = ax2, color = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f267a14",
   "metadata": {},
   "source": [
    "- As we have one observation per sample, the mean is the observation, so the mean can be wherever in the interval 0 and 11.\n",
    "- Let's do it for sample size equal 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dedfdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 10000 simulated experiments with 1 subject each, and calculate the sample mean for each experiment\n",
    "sample_size = 2\n",
    "beta_hats = []\n",
    "for i in range(1,10000):\n",
    "    beta_hat = st.mean(np.random.normal(loc = beta, scale = sigma, size = sample_size).astype(int))\n",
    "    beta_hats.append(beta_hat)\n",
    "\n",
    "\n",
    "# plot a histogram of the distribution of sample means, together with the population distribution\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(beta_hats, ax = ax, binwidth=1)\n",
    "ax2 = ax.twinx()\n",
    "sns.lineplot(x = pdf_range, y = pdf_dens, ax = ax2, color = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ac7b8",
   "metadata": {},
   "source": [
    "- Already better, the simulated sampling distribution is telling us that with a sample of 2 the mean may range between 2 and 9. Not great, but better.\n",
    "- Let's see with a sample size of 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c209e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 10000 simulated experiments with 1 subject each, and calculate the sample mean for each experiment\n",
    "sample_size = 10\n",
    "beta_hats = []\n",
    "for i in range(1,10000):\n",
    "    beta_hat = st.mean(np.random.normal(loc = beta, scale = sigma, size = sample_size).astype(int))\n",
    "    beta_hats.append(beta_hat)\n",
    "\n",
    "\n",
    "# plot a histogram of the distribution of sample means, together with the population distribution\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(beta_hats, ax = ax, binwidth=1)\n",
    "ax2 = ax.twinx()\n",
    "sns.lineplot(x = pdf_range, y = pdf_dens, ax = ax2, color = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524b6401",
   "metadata": {},
   "source": [
    "- As the sample size increases, the values that the sample mean can take is narrower and narrower around the population value.\n",
    "- The bigger the sample size, the narrower the sampling distribution gets. We can quantify this effect by calculating the standard deviation of the sample mean, which is referred to as the **standard error**, which will go down as the sample size increases. We'll see it in what follows.\n",
    "- Another important thing that we will not show here is that, no matter what shape our population distribution is, as $N$ increases the sampling distribution of the sample mean starts to look more like a normal distribution. So we can have a population with a very weird distribution, but the sampling distribution of the sample mean will always be a Normal as $N$ goes up. Already for sample size > 10, the simulated sampling distribution looks already a lot like a normal (we could prove it with simulations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7715448",
   "metadata": {},
   "source": [
    "- So it seems like we have evidence for all of the following claims about the sampling distribution of the sample mean:\n",
    "    - The mean of the sample mean is the same as the mean of the population\n",
    "    - The standard deviation of the sample mean (i.e., the standard error) gets smaller as the sample size increases\n",
    "    - The shape of the sampling distribution of the sample mean becomes normal as the sample size increases\n",
    "- As it happens, not only are all of these statements true, there is a very famous theorem in statistics that proves all three of them, known as **the central limit theorem**. Among other things, the central limit theorem tells us that if the population distribution has mean $\\beta$ and standard deviation $\\sigma$, then the sample mean also has mean $\\beta$, and the standard error of the sample mean (SEM) is $\\frac{\\sigma}{\\sqrt{N}}$\n",
    "- So if we assume that the random variable $y$ is normally distributed with mean $\\beta$ and variance $\\sigma^2$, or with any distribution in large enough samples, we can say that the sample mean is normally distributed with the following PDF and moments:\n",
    "<br><br>\n",
    "$$f(\\hat{\\beta})=\\frac{1}{\\frac{\\sigma}{N}\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{\\hat{\\beta}-\\beta}{\\frac{\\sigma}{N}}\\right)^{2}\\right)$$\n",
    "<br>\n",
    "$$E(\\hat{\\beta}) = \\beta$$\n",
    "<br>\n",
    "$$V(\\hat{\\beta}) = \\frac{\\sigma^2}{N}$$\n",
    "<br>\n",
    "$$\\hat{\\beta} \\sim \\mathcal{N}(\\beta,\\,\\frac{\\sigma^2}{N})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46ab8c0",
   "metadata": {},
   "source": [
    "- Similar reasoning apply for the sample variance, with some differences. First the estimator sample variance per se is a biased estimator, and to make it unbiased we need to scale it by $N-1$. Second, the estimator sample variance follows a $\\chi^2$-distribution. A $\\chi^2$-distribution with k degrees of freedom is the distribution of a sum of the squares of k independent standard normal random variables ([Wikipedia, Chi distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution)).\n",
    "- Formally:\n",
    "<br><br>\n",
    "$$\\hat{\\sigma}^2=\\frac{1}{N-1}\\sum_{i=1}^{N}(y_i-\\hat{\\beta})^2$$\n",
    "<br>\n",
    "$$\\frac{N-1}{\\sigma^2}\\hat{\\sigma}^2=\\hat{\\sigma}^{2}_{a} \\sim \\mathcal{\\chi^2_{N-1}} $$\n",
    "<br>\n",
    "$$f(\\hat{\\sigma}^2_a) = \\frac{{\\hat{\\sigma}^2_a}^{(N-1)/2-1}e^{-{\\hat{\\sigma}^2_a}/2}}{2^{(N-1)/2}\\kern 0.08 em \\Gamma((N-1)/2)} $$\n",
    "<br>\n",
    "$$E(\\hat{\\sigma}^2)=E(\\frac{\\sigma^2}{N-1}\\chi^2_{N-1})=\\sigma^2$$\n",
    "<br>\n",
    "$$V(\\hat{\\sigma}^2)=V(\\frac{\\sigma^2}{N-1}\\chi^2_{N-1})=\\frac{\\sigma^4}{(N-1)^2}V(\\chi^2_{N-1})=\\frac{2\\sigma^4}{N-1}$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdad9f4",
   "metadata": {},
   "source": [
    "### Confidence Intervals <a name=\"Confidence-Intervals\"></a>\n",
    "- The thing that we did not talk about is an attempt to quantify the amount of uncertainty that attaches to our estimate. For example, it would be nice to be able to say that there is a 95% chance that the true mean of the CO2 emissions per capita lies between 1 and 9. The name for this is a confidence interval for the mean.\n",
    "- Suppose the true population mean is $\\beta$ and the standard deviation is $\\sigma$. We’ve just finished running our study that has N participants, and the mean of CO2 emissions per capita among those participants is $\\hat{\\beta}$. We know from our discussion of the central limit theorem that the sampling distribution of the mean is approximately normal. We also know from our discussion of the normal distribution that there is a 95% chance that a normally-distributed quantity will fall within approximately two standard deviations of the true mean (i.e. between 1 and 9).\n",
    "- We can use the `norm.ppf()` function from `scipy.stats` to compute the 2.5th and 97.5th percentiles of the normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e794678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm.ppf([.025, 0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30675d6",
   "metadata": {},
   "source": [
    "- Yes so 95% of the values for a normally-distributed random variable lies between -1.96 and 1.96 standard deviations from the mean. \n",
    "- Recall that the expected value of the sample mean is $\\beta$ and that the standard deviation of the sampling distribution of the sample mean is referred to the standard error of the mean (SEM).\n",
    "- So we can be 95% confident that the realization of the sample mean will fall between -1.96 and 1.96 SEM from its mean. This statement can be expressed as follows:\n",
    "<br>\n",
    "$$\\beta - (1.96 \\times SEM) \\leq \\hat{\\beta} \\leq \\beta + (1.96 \\times SEM)$$\n",
    "<br>\n",
    "- The other way around, this implies that the range of values $[\\hat{\\beta} - (1.96 \\times SEM), \\hat{\\beta} + (1.96 \\times SEM)]$ has a 95% probability of containing the population mean $\\beta$:\n",
    "<br>\n",
    "$$\\hat{\\beta} - (1.96 \\times SEM) \\leq \\beta \\leq \\hat{\\beta} + (1.96 \\times SEM)$$\n",
    "<br>\n",
    "- We refer to this range as a 95% confidence interval, denoted $CI_{95}$. In short, as long as N is sufficiently large – large enough for us to believe that the sampling distribution of the mean is normal – then we can write this as our formula for the **95% confidence interval**:\n",
    "$$CI_{95} = \\hat{\\beta}\\pm(1.96\\times\\frac{\\sigma}{\\sqrt{N}})$$\n",
    "- When we do not know the parameter $\\sigma$, and in most of the cases we don't, we have to use the estimate coming from the estimator $\\hat{\\sigma}^2$. This is pretty straightforward to do, but this has the consequence that we need to use the quantiles of the t-distribution rather than the normal distribution to calculate our magic number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f19169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.t.ppf([.025, 0.975], N-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d370a58e",
   "metadata": {},
   "source": [
    "### Hypothesis Testing <a name=\"Hypothesis-Testing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15172a52",
   "metadata": {},
   "source": [
    "### Some reminders of testing\n",
    "- A **hypothesis** is a statement about a population parameters. The formal testing procedure involves a statement of the hypothesis, usually in terms of a \"null\" and an \"alternative\", conventionally denoted $H_0$ and $H_1$.\n",
    "- We need to make a decision about whether to believe that the null hypothesis is correct, or to reject the null hypothesis in favour of the alternative. The name for the thing that we calculate to guide our choices is called a **test statistic**.\n",
    "- Having chosen a test statistic, the next step is to state precisely which values of the test statistic would cause us to reject the null hypothesis, and which values would cause us to keep it. In order to do so, we need to determine what the **sampling distribution of the test statistic** would be if the null hypothesis were actually true.\n",
    "- **Type I error**: Reject of a true (null) hypothesis.\n",
    "- The single most important design principle of the test is to control the probability of a type I error, to keep it below some fixed probability. This probability, which is denoted $\\alpha$, is called the **significance level** of the test (or sometimes, the size of the test). So a hypothesis test is said to have significance level $\\alpha$ if the type I error rate is no larger than $\\alpha$. Note that $\\alpha$ is fixed, i.e. 0.05.\n",
    "- The **p-value** is defined to be the smallest Type I error rate that you have to be willing to tolerate if you want to reject the null hypothesis. The p-value is calculated.\n",
    "- The **critical region** of the test corresponds to those values of the test statistic that would lead us to reject the null hypothesis (which is why the critical region is also sometimes called the rejection region).\n",
    "- Same conclusions can be drawn by using the confidence interval (rather than values of test statistics and p values).\n",
    "- For more info, check the chapter [Hypothesis Testing](https://ethanweed.github.io/pythonbook/04.04-hypothesis-testing.html) by Weed and Navarro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc33b60",
   "metadata": {},
   "source": [
    "### The one-sample z-test (unknown mean and known variance)\n",
    "- Our null hypothesis, $H_0$, is that the true population mean $\\beta$ for CO2 emissions per capita is 5 tonnes; and our alternative hypothesis, $H_1$, is that the population mean isn’t 5 tonnes. If we write this in mathematical notation, these hypotheses become\n",
    "<br><br>\n",
    "$$H_0:\\beta=5$$\n",
    "<br>\n",
    "$$H_1:\\beta \\neq 5$$\n",
    "- There are two special pieces of information that we can add: the CO2 emissions per capita are normally distributed, the true standard deviation of CO2 emissions per capita $\\sigma$ is known to be 2.\n",
    "- The null hypothesis $H_0$ and the alternative hypothesis $H_1$ can be illustrated like this (here $\\mu$ is our $\\beta$):\n",
    "<img src=\"https://i.ibb.co/TmxZjft/Screen-Shot-2023-10-10-at-19-58-46.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f7d486",
   "metadata": {},
   "source": [
    "- If the null hypothesis is true (\"under the null hypothesis\"), our random variable $y$ for CO2 emissions per capita is normally distributed as follows:\n",
    "<br><br>\n",
    "$$ y \\sim \\mathcal{N}(\\beta_0,\\,\\sigma^2)$$\n",
    "<br>\n",
    "- So under the null the sample mean is normally distributed as follows:\n",
    "$$\\hat{\\beta} \\sim \\mathcal{N}(\\beta_0,\\,\\frac{\\sigma^2}{N})$$\n",
    "<br>\n",
    "- Now comes the trick. What we can do is convert the sample mean $\\hat{\\beta}$ into a standard score, i.e. a test statistic, which has the following formula:\n",
    "$$ z_{\\hat{\\beta}} = \\frac{\\hat{\\beta}-\\beta_0}{\\sigma/\\sqrt{N}} $$\n",
    "<br>\n",
    "- This z-score, our test statistic, follows a standard normal distribution:\n",
    "$$ z_{\\hat{\\beta}} \\sim \\mathcal{N}(0,\\,1)$$\n",
    "<br>\n",
    "- Regardless of what the population parameters for the raw scores actually are, the 5% critical regions for z-test are always the same:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa70b9b",
   "metadata": {},
   "source": [
    "<img src=\"https://i.ibb.co/gDsNWnq/Screen-Shot-2023-10-10-at-19-58-58.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2c47d",
   "metadata": {},
   "source": [
    "- OK let's now do this with Python on our randomly drawn sample of CO2 emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "N = 100\n",
    "draws_co2 = np.random.normal(loc = beta, scale = sigma, size=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e9d4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_hat = st.mean(draws_co2)\n",
    "beta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f2f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_true = 2\n",
    "beta_null = 5\n",
    "\n",
    "N = len(draws_co2)\n",
    "\n",
    "sem_true = sd_true / np.sqrt(N)\n",
    "\n",
    "z = (beta_hat - beta_null) / sem_true\n",
    "z.round(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf44a87",
   "metadata": {},
   "source": [
    "- We can see that 0.34 is lower than the critical value of 1.96 that is required to be significant at $\\alpha=0.5$. Under the null hypothesis, the z-statistic distributes like a Normal with mean 0 and variance 1, which means that in 95% of the cases the z-statistic will have a value between -1.96 and 1.96. In our specific case (with out specific sample), it does, as $-1.96<0.34<1.96$. This means that, based on the value we have for the z-statistic, we cannot reject our null hypothesis (what we found is in line with the distribution implied by the null hypothesis).\n",
    "- We can arrive to a similar conclusion using the p-value of the t-statistic. Notice that the $\\alpha$ level of a $z$-test (or any other test, for that matter) defines the total area \"under the curve\" for the critical region. That is, if we set $\\alpha = .05$ for a two-sided test, then the critical region is set up such that the area under the curve for the critical region is $.05$. And, for the $z$-test, the critical value of 1.96 is chosen that way because the area in the lower tail (i.e., below $-1.96$) is exactly $.025$ and the area under the upper tail (i.e., above $1.96$) is exactly $.025$. So, since our observed $z$-statistic is $0.34$, let's calculate the area under the curve below $-0.34$ or above $0.34$. In Python we can calculate this using the `NormalDist().cdf()` method. For the lower tail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25799dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_area = st.NormalDist().cdf(-abs(z))\n",
    "round(lower_area,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950b437a",
   "metadata": {},
   "source": [
    "- Since the normal distribution is symmetrical, the upper area under the curve is identical to the lower area, and we can simply add them together to find our exact p-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb288a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_area = lower_area\n",
    "p = lower_area + upper_area\n",
    "round(p,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc3fb9f",
   "metadata": {},
   "source": [
    "- As the p-value is larger than 0.05, we can say that, based on our estimate and sample, the mean of CO2 emissions per capita (5.07 tonnes), is not statistically different than 5.\n",
    "- Another way of doing the same thing is computing the 95% confidence interval, and checking if the interval contains 5. We can also do a similar test with $\\beta_0=0$. Finally, we could do a similar test with unknown variance and the t-distribution. These exercises are covered in the notebook \"probability_exercises\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f0029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
