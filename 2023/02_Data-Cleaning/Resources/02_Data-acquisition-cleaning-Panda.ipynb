{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0e1e41-0543-4373-adfd-57e155d8a2d0",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/edoardochiarotti/class_datascience/blob/main/2023/02_Data-Cleaning/Resources/02_Data-acquisition-cleaning-Panda.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9980e5b5-4dd8-44b4-9af3-8a72da9a4494",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Statistics and Data Science: Data acquisition and cleaning with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bf30d7-06c0-4284-b735-3810197cafa4",
   "metadata": {},
   "source": [
    "<img src='https://miro.medium.com/v2/resize:fit:720/format:webp/0*hHVINI5TGJB6jPKN.jpg' width=\"600\">\n",
    "\n",
    "Source: Roman Orac [Pandas Web API - Towards Data Science](https://towardsdatascience.com/pandas-analytics-server-d64d20ef01be)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96027b29-1c6b-47ca-acee-10e8b165082f",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "- [Library, Packages and Modules](#Library,-Packages-and-Modules)\n",
    "   - [Import packages](#Import-packages)\n",
    "   - [Style Guide for Python Code](#Style-Guide-for-Python-Code)\n",
    "- [Pandas](#Pandas)\n",
    "   - [Importing data](#Importing-data)\n",
    "   - [Discovering your data frame](#Discovering-your-data-frame)\n",
    "      - [Dimensions of data frame](#Dimensions-of-data-frame)\n",
    "      - [Data frame indexing](#Data-frame-indexing)\n",
    "      - [Scope: data types](#Scope:-data-types)\n",
    "      - [Scope: Extract unique values in a column](#Scope:-Extract-unique-values-in-a-column)\n",
    "   - [Cleaning your data frame](#Cleaning-your-data-frame)\n",
    "      - [Identifying NaN](#Identifying-NaN)\n",
    "      - [Droping NaN](#Droping-NaN)\n",
    "      - [Dealing with errors and other missing values](#Dealing-with-errors-and-other-missing-values)\n",
    "   - [Merging data frames](#Merging-data-frames)\n",
    "   - [Manipulating your data](#Manipulating-your-data)\n",
    "      - [Operating on columns](#Operating-on-columns)\n",
    "      - [Functions and data frame](#Functions-and-data-frame)\n",
    "      - [Calculating GrDP](#Calculating-GrDP)\n",
    "   - [Exporting data frame](#Exporting-data-frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976528f0-12fb-44c1-ae03-818350717c41",
   "metadata": {},
   "source": [
    "## Library, Packages and Modules <a name=\"Library,-Packages-and-Modules\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f78dd1-7d47-4d15-a9f6-83fe74e9728b",
   "metadata": {},
   "source": [
    "Until now, we have operated on our data using built-in functions, operators, and objects' methods. We already performed quite neat operations thanks to the Python Standard Library, which has lots of built-in modules that contain useful functions and data types for doing specific tasks. But we can also use modules from outside the standard library, and we can even write our own modules!\n",
    "\n",
    "A **module** is contained in a file that ends with `.py`. This file can have **classes**, functions, and other objects. We will not discuss classes for now, just remember that a class is like an object constructor, or a \"blueprint\" for creating objects (Check the [Documentation](https://docs.python.org/3/tutorial/classes.html) to learn more and a nice introduction from [GeeksforGeeks](https://www.geeksforgeeks.org/python-classes-and-objects/)).\n",
    "\n",
    "A **package** contains several related modules that are all grouped together under one name. For instance, [Pandas](http://pandas.pydata.org) (derived from \"panel data\") is the go-to package for data analysis and manipulation. Another fundamental package for scientific computing is [NumPy](http://www.numpy.org) (Numerical Python).\n",
    "\n",
    "A **library** is an umbrella term referring to a reusable chunk of code, which usually contains a collection of related modules and packages. For instance, [Matplotlib](https://matplotlib.org/) is a comprehensive library for creating static, animated, and interactive visualizations. In practice, library and package are often used interchangeably.\n",
    "\n",
    "Standard Python installations come with the standard library. Outside of the standard library, there are several packages available such as pandas and NumPy. Several. Ha! There are currently more than 300,000 packages available through the [Python Package Index](https://pypi.python.org/pypi), PyPI! Usually, you can ask Google about what you are trying to do, and there is often a third party module to help you do it. The most useful (for scientific computing) and thoroughly tested packages and modules are available using `conda`. Others can be installed using `pip`.\n",
    "\n",
    "We will discover several other packages along our journey, but for now let's discover how to access and use packages and modules.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f523293-d3ca-41de-a3f0-7c1ea0279cc7",
   "metadata": {},
   "source": [
    "### Import packages <a name=\"Import-packages\"></a>\n",
    "\n",
    "To access a package, we have to `import` it. For instance, let's import the `numpy` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349bbd5c-d5db-4e65-83cc-2e8318b6d3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef04f42-2bd6-4754-a4cb-2138b62632e3",
   "metadata": {},
   "source": [
    "That's it! Now we can start using the numerous functionalities offered by `numpy` such as means, medians, standard deviations, and lots and lots and lots of other numerical operations. Let's explore what is available in `numpy`. Remember, in Python everything is an object, so if we want to access the methods and attributes available in `numpy`, we use dot syntax. In Colab, we can type `numpy.` and then hit tab to discover what the module offers, Note that this technique works for all objects, so do not hesitate to use it when, for instance, you do not remember the name of a method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f82ae04-2d25-4de1-9ef6-553a62a09180",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e80733-1d25-48d2-ad75-9c552eed30a5",
   "metadata": {},
   "source": [
    "That's a lot of options! Let's try the `numpy.mean()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9b2c5-b3e3-458e-bbab-84e0dc72cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lis = [1,2,3,4,5,6]\n",
    "\n",
    "numpy.mean(my_lis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c066ed-2051-4add-a7bb-2531c0866c81",
   "metadata": {},
   "source": [
    "Nice! Let's try the `numpy.median()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e97aeb-b577-4b0e-a6b3-4789b8b29f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.median(my_lis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43de25c-d0c3-4de4-a395-4029fbe148db",
   "metadata": {},
   "source": [
    "This is cool. It gives the median, including when we have an even number of elements in the sequence of numbers, in which case it automatically interpolates. It is really important to know that it does this interpolation, since, if you are not expecting it, it can give unexpected results. So, here is an important piece of advice:\n",
    "\n",
    "<div style=\"color: dodgerblue; text-align: center; font-weight: bold;\">\n",
    "\n",
    "Always check the doc strings of functions.    \n",
    "\n",
    "</div>\n",
    "\n",
    "We can access the doc string of the `numpy.median()` function by typing `numpy.median?`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f93ad-30b1-4300-8cef-673aaa627d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.median?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22715751-5bcd-48a2-a4e4-43d47674e675",
   "metadata": {},
   "source": [
    "See in the output: \n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Given a vector ``V`` of length ``N``, the median of ``V`` is the\n",
    "    middle value of a sorted copy of ``V``, ``V_sorted`` - i\n",
    "    e., ``V_sorted[(N-1)/2]``, when ``N`` is odd, and the average of the\n",
    "    two middle values of ``V_sorted`` when ``N`` is even.\n",
    "\n",
    "This is where the documentation tells you that the median will be reported as the average of two middle values when the number of elements is even. Note that you could also read the [median documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.median.html), which is a bit easier to read. And you can check the [Numpy Documentation](https://numpy.org/doc/stable/) to discover the full extent of `numpy` power! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3046ca95-b1d0-4825-9b11-7f0c96fd0db3",
   "metadata": {},
   "source": [
    "As you can see, `numpy`and other modules are super useful and we will use then all the time. There is a drawback: we always have to use the dot syntax with the full name of the module to access the methods it contains, and typing `numpy` over and over again can get annoying... Wait a minute, you do not actually have to do that! We can use the `as` keyword to import a module as an **alias**. Numpy's alias is traditionally `np`, so you shall always use this alias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40141713-a52d-4804-8436-5893c5449bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean([1.1, 8.4, 5.3, 6.7, 9.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ec57bc-c633-466c-be39-0891b6fe8a2a",
   "metadata": {},
   "source": [
    "Finally, you do not have to import the full package/module if you want to use only a specific element. For example, suppose we need the value of pi, which can be accessed via the `math` module ([Documentation](https://docs.python.org/3/library/math.html). We could do as before, importing the full module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81cd7bc-cd7f-4a5c-8f83-11f207102128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "math.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2b31f5-10ad-4d5a-9850-fd184fecf91a",
   "metadata": {},
   "source": [
    "Alternatively, we only import `pi`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5297988-9eed-4f61-8b03-26b77ed4339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dbd9d6-72c5-4555-bd40-baac714db271",
   "metadata": {},
   "source": [
    "Amazing, when using from-import, we do not need to use the dot syntax! Indeed, in this example, we did not import the full module, rather just `pi` as a variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca61e9-6da8-4491-9c6a-20985bf339ee",
   "metadata": {},
   "source": [
    "Packages and modules are super convenient. If you want to do something that seems really common, a good programmer (or a team of them) probably already wrote something to do that. So always check online what is there before jumping in into coding complex pieces of codes! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6753420f-40c0-4d4a-987d-cf3a35eb5032",
   "metadata": {},
   "source": [
    "### Style Guide for Python Code <a name=\"Style-Guide-for-Python-Code\"></a>\n",
    "\n",
    "There are some good practices when writing Python code. A great Python style guide is [PEP 8](https://www.python.org/dev/peps/pep-0008/). Here is the recommendation about importing libraries, packages, and modules:  \n",
    "\n",
    ">Imports are always put at the top of the file, just after any module comments and docstrings, and before module globals and constants.\n",
    ">\n",
    ">Imports should be grouped in the following order:\n",
    ">\n",
    ">1. standard library imports\n",
    ">2. related third party imports\n",
    ">3. local application/library specific imports\n",
    ">\n",
    ">You should put a blank line between each group of imports.\n",
    "\n",
    "Try to follow this guide!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57ce5c3-ecf3-400c-a9c8-4ec23e946080",
   "metadata": {},
   "source": [
    "## Pandas <a name=\"Pandas\"></a>\n",
    "\n",
    "Throughout your career, you will undoubtedly need to handle data, possibly lots of data. Data comes in lots of formats, and you will spend much of your time manipulating and cleaning it to obtain usable form for analysis.\n",
    "\n",
    "As mentioned before, Pandas (derived from \"panel data\") is the go-to package for data analysis and manipulation. Its primary object, the `DataFrame` is extremely useful in wrangling data. We will explore some of that functionality here, and will put it to use in all along this course.\n",
    "\n",
    "You can read more about Pandas in the [documentation](https://pandas.pydata.org/docs/index.html), and you can refine your knowledge with this [online course and tutorial](https://realpython.com/learning-paths/pandas-data-science/). Since Pandas is one of the most used package, you can also find a ton of material online, answering any questions you might have. As always, there is no need to reinvent the wheel, and you should rely on the years of experience and knowledge of programmers who already faced similar issues you might encounter. \n",
    "\n",
    "Without further ado, let's import Pandas. We generally `import pandas as pd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14c2985-90db-47ce-b1aa-15f976e42430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9836d35c-09d1-41d9-b65d-9bd8178f542f",
   "metadata": {},
   "source": [
    "### Importing data <a name=\"Importing-data\"></a>\n",
    "\n",
    "Now the fun begins! We will discover the functionalities offered by Pandas using \"real\" data. More precisely, we will do an application on the Green Domestic Product (GrDP).\n",
    "\n",
    "GrDP is a novel indicator developed by E4S to remedy some of the shortcomings of GDP. GrDP extends the scope of the GDP to integrate the depletion of natural, social, and human capital. In its current version, GrDP considers the impacts of the emissions of three groups of pollutants: greenhouse gases, air pollutants, and heavy metals. These impacts include climate change, health issues, decrease in crops' yields and biomass production, buildings degradation, and damages to ecosystems due to eutrophication. \n",
    "\n",
    "To learn more, you can read the recent E4S white paper applying GrDP to Switzerland and the methodological report [here](https://e4s.center/en/resources/reports/green-domestic-product/). The IbyIMD magazine also published an [article](https://iby.imd.org/sustainability/lets-replace-gdp-introducing-the-green-domestic-product/) to introduce GrDP and its application for policymakers and businesses. Finally, you can explore the results of the project in our [online dashboard](https://public.tableau.com/app/profile/jordane.widmer/viz/GrDP-InteractiveInterface/Tableaudebord1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b7f537-e74d-4b61-ab11-1e678b98392c",
   "metadata": {},
   "source": [
    "First, we need to import our data. The datasets were uploaded to our GitHub repository as two CSV files: \"GrDP_Panel-Data.csv\" and \"GrDP_Cost.csv\". We can directly import online CSV file using the `.read_csv()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703a328-cdc2-4a63-a4b1-c16dd4fadb24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/edoardochiarotti/class_datascience/main/2023/02_Data-Cleaning/data/GrDP_Panel-Data.csv\"\n",
    "url_cost = \"https://raw.githubusercontent.com/edoardochiarotti/class_datascience/main/2023/02_Data-Cleaning/data/GrDP_Cost.csv\"\n",
    "\n",
    "df=pd.read_csv(url)\n",
    "df_cost = pd.read_csv(url_cost)\n",
    "\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3014520a-f2d9-434f-bada-9c18dc276b2f",
   "metadata": {},
   "source": [
    "Note that to access the data url in GitHub, you need to click on your data file, and then click on \"Raw\".\n",
    "\n",
    "You can import data from a variety of sources e.g., Excel file, TSV, HTML, JSON, etc., define specific columns to import, and even rename them! Learn more [how to import data into Pandas dataframes](https://practicaldatascience.co.uk/data-science/how-to-import-data-into-pandas-dataframes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554da718-f471-493a-92e2-6acd8389ade3",
   "metadata": {},
   "source": [
    "### Discovering your data frame <a name=\"Discovering-your-data-frame\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9562f-8458-4708-99e2-f4c8f8751c3a",
   "metadata": {},
   "source": [
    "The first thing you want to do is explore your data frame to understand its structure and what it contains. In a notebook, we can directly look at it in a nice, visual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba69bd89-98d2-421c-95be-9e03802dc860",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d133076-1071-4542-8003-471a689ea8d1",
   "metadata": {},
   "source": [
    "This is a nice representation of the data, but we really do not need to display that many rows of the data frame in order to understand its structure. Instead, we can use the `.head()` method of data frames to look at the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04bc7e5-4398-4efc-9d2b-44cb595490bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc8235b-b518-4c91-a9f3-bf2d7f513a9e",
   "metadata": {},
   "source": [
    "Nice! We can see that the data includes the GDP, population, and emissions of various pollutants for several countries and years. Let's continue our exploration!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8503356-ba8e-44c0-87cb-ec4f2b3210ef",
   "metadata": {},
   "source": [
    "#### Dimensions of data frame <a name=\"Dimensions-of-data-frame\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28869473-7f21-4664-9d9c-6ddfe6c3efc6",
   "metadata": {},
   "source": [
    "When we look at our whole data frame, the dimensions of our data frame were printed below the table (900 rows x 17 columns). We can also directly extract the number of observations (rows) using the `len()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764d8bdf-9f77-41b1-8203-4033d9fceee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af800339-fddc-4f5a-bafb-97fbc7c09e6d",
   "metadata": {},
   "source": [
    "Similarly, using the `len()` function and the `.columns` method, we have access to the number of variables (columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4e03c7-cb80-4237-85bb-dd6f24f59ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44cb778-0e99-4de8-b46f-e43160c05f2b",
   "metadata": {},
   "source": [
    "We can also use directly return the number of observations and variables using the `.shape` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f735b7bc-6218-4ea5-ab7f-23270b37d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2d4913-b106-4239-885d-cdd38efd0512",
   "metadata": {},
   "source": [
    "Going on with our exploration, we can print the names of the columns. As often there are several ways depending on what we want to achieve. Here are some options:\n",
    "- `list(df.columns)` returns a list of our columns,\n",
    "- `sorted(df)` returns a sorted list of our columns,\n",
    "- `df.keys()` returns a pandas Index object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece0249e-e01e-4ce5-972d-3a2ff8da4bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34a57c2-8d4d-42ce-bdd8-6a44ffd353ab",
   "metadata": {},
   "source": [
    "#### Data frame indexing <a name=\"Data-frame-indexing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511481d2-c2a6-4466-b87a-f4736bce0cb8",
   "metadata": {},
   "source": [
    "Once we have understood the structure of our data frame, it is a good idea to look at some observations. \n",
    "\n",
    "We **index data frames by columns**. For instance, let's have a look at countries' population:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a3fee3-924f-4389-9970-8a86981e9ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Population']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4852beb-dde8-401a-9549-58829b045b33",
   "metadata": {},
   "source": [
    "What if we want to extract a specific value? Well, did you notice on the left of our data frame, there was a nameless column. Nameless but not useless, this column is the row labels. \n",
    "\n",
    "We can use the label to extract a given value. For example, let's say we want to extract the population of Belgium in 1990 (first row), one way is to use the syntax `df['Population'][0]`. However, the preferred way is to use the `.loc` method, short for location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90c5cd1-1fec-4b09-9373-7db2f78833a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0, 'Population']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8485e563-8ea4-4ce3-9a84-239bc33c424d",
   "metadata": {},
   "source": [
    "What if we want several values, say population and GDP? Easy! We can use lists specifying the labels of row and columns to extract subset of our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e3ad7e-584f-4129-9f1e-78074aae2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[[3,87], ['Country', 'Year','GDP [million Euro]','Population']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46ad0d-0946-4a8a-a2b9-1991a45b7db1",
   "metadata": {},
   "source": [
    "We can even extract an entire row with `.loc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa85927a-a7a0-404d-a63e-3cd869d25f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd0a3ef-b455-4587-b0fe-ed864e72cfcd",
   "metadata": {},
   "source": [
    "Remember when we did list slicing? We can do similar operations here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc8b0e5-ef7b-4cdb-bd9b-e5555b07d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1:10:2, 'Country':'Population']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b2338a-56de-4fe6-bf11-a84f255f91e1",
   "metadata": {},
   "source": [
    "Now imagine that we want to use numerical index for columns, instead of labels. Well, there is a way! We can use the`.iloc` method to index data frames based on the integer-location rows and columns. As always in Python, indexing starts at 0. Let's extract the same data subset as before, this time using `iloc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b568c-309d-4e0e-9e89-9870c13f6cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[1:10:2, 0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bee218b-19e0-448c-a485-22e213bd280b",
   "metadata": {},
   "source": [
    "As a parenthesis, note that in our case, the label column is using integers - this is the default when importing data. Hence, extracting rows with `.loc` and `iloc` is using the same syntax... Almost:\n",
    "- `.loc` gets rows and columns with particular **labels** (in our case, integers)\n",
    "-`.iloc` gets rows and columns at integer **locations**\n",
    "\n",
    "How does it affect us? Well, one difference is slicing. While `.iloc` works the same as slicing lists, `.loc` is a little bit different. For example, if we pass as argument `[0:1]`, `.iloc` would only return the first row (as with lists, slicing excludes the last element), but `.loc` would return the two rows with labels 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcce41b-38fb-4955-93a7-8c8428955828",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d5caf0-ce89-4ac7-9a2b-92b120ede770",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed7b96-1eff-4e4b-ad9e-52527c8073af",
   "metadata": {},
   "source": [
    "Similarly, `df.iloc[-1]` would allow us to access the last line, but `df.loc[-1]` would yield an error since there is no row labeled '-1'. For a more extensive discussion of the differences between `.loc` and `.iloc`, see this [Stack Overflow post](https://stackoverflow.com/questions/31593201/how-are-iloc-and-loc-different#:~:text=The%20main%20distinction%20between%20the,or%20columns%29%20at%20integer%20locations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23654718-20ed-4a4e-8552-849e064f6625",
   "metadata": {},
   "source": [
    "Both methods, `.loc` and `.iloc`, are very helpful, but you need to know the label or location of your data. Now what if we want to access data based on a given column value. For instance, what if we want to extract data only for Switzerland? We know neither the labels nor the locations of the rows corresponding to Switzerland... Such operation seems super common - for example you could imagine having company/customer data with some ID numbers or names - so there must be a way. Indeed, there is! We can use a condition:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc62abd-e689-42d5-9535-76ee08e671e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Country'] == 'Switzerland']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31d749-add5-451d-b95a-bd2fd78cebff",
   "metadata": {},
   "source": [
    "Note the syntax: \n",
    "- `df['Country']` extracts our column labeled 'Country' ,\n",
    "- `df['Country'] == 'Switzerland'` returns a column of `True` if the country is Switzerland, and `False` otherwise,\n",
    "- When feeding this condition to our data frame `df`, it will only keep the rows for which the condition is `True`\n",
    "\n",
    "Let's try another one, this time extracting data for Switzerland in 2019. We can use `&` to include a second condition. We did not cover this **bitwise operator** before, but the syntax is self-explanatory in the example below. Note that it is important that each Boolean operation you are doing is in parentheses because of the precedence of the operators involved. \n",
    "\n",
    "In pandas, the bitwise operators perform element-wise comparisons. That's what we want to do here: find **for each row** if the country is Switzerland and if the year is 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec2fae4-1744-4a77-91e9-fe7356605755",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Country'] == 'Switzerland') & (df['Year'] == 2019)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c61963-a416-4c5b-9687-e52dd0251d3a",
   "metadata": {},
   "source": [
    "We can also select some columns, but in this case, we also need to use the `.loc` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf6d70-3f09-43c9-855f-0573505ae1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['Country'] == 'Switzerland') & (df['Year'] == 2019), ['GDP [million Euro]','Emissions_GHG [thousand tonnes CO2eq]']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a486be44-0410-47ca-81e4-7c0b94084e8c",
   "metadata": {},
   "source": [
    "Or a slice of columns, for instance all the emissions of air pollutants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6ef2a3-390b-4c49-9645-c889dc8f5857",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['Country'] == 'Switzerland') & (df['Year'] == 2019), 'Emissions_NOx [tonne]':'Emissions_NH3 [tonne]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d95b19c-544d-4b13-825d-d5eb8a86413b",
   "metadata": {},
   "source": [
    "#### Scope: data types <a name=\"Scope:-data-types\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b11b69-f61d-4731-915f-3ef94a497b48",
   "metadata": {},
   "source": [
    "Let's keep on exploring our data frame. We can identify the types of our variables using the method `.dtypes`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47939be-6223-4153-8237-311f356aea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141cc36e-b61d-4665-b018-ded138d6b79c",
   "metadata": {},
   "source": [
    "As you can see, Pandas is using different names for data types. Here is a description:\n",
    "\n",
    "|Pandas type|Native Python type|Description|\n",
    "|:-------|:-------|:----------|\n",
    "|`object` | `string` | The most general dtype. Will be assigned to your column if column has mixed types (numbers and strings). |\n",
    "|`int64` | `int` | Numeric characters. 64 refers to the memory allocated to hold this character. |\n",
    "|`float64` | `float` | Numeric characters with decimals. If a column contains numbers and NaNs (see below), pandas will default to float64, in case your missing value has a decimal.|\n",
    "\n",
    "Our data frame contains `object` (e.g., strings like countries), `int64` and `float64`. You may wonder about the emissions of arsenic (As), nickel (Ni), and chromium (Cr), which are apparently `object` type. We will discover why later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b893c-480f-4b17-acb2-5a222ad08c2a",
   "metadata": {},
   "source": [
    "#### Scope: Extract unique values in a column <a name=\"Scope:-Extract-unique-values-in-a-column\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553bb92-a2c0-4541-b5f5-9989c7d788f2",
   "metadata": {},
   "source": [
    "From the column labels of our data frame, we know that we have data for various countries and years. For example, we have seen before that we have data for Switzerland between 1990 and 2019. Which other country-years are included in our data frame? We can use the `.unique()` method to return the unique values of a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1465f51e-8e9d-4d8e-b85e-33355f45fb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Country'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76a5f74-4278-4b9f-b6f0-390b32989fb3",
   "metadata": {},
   "source": [
    "Ok! So we have for European countries. What about years? We can use the same syntax, but let's discover a new trick! As everything in Python, `df['Country']` is an object, and we apply the `.unique()` method to this object. Instead of `df['Country']`, we can use the equivalent statement `df.Country`. Let's try with the 'Year' column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e0217-dca2-469e-84b7-0e4353852202",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Year.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ee84cc-44d7-4cbb-b259-6c83220aac06",
   "metadata": {},
   "source": [
    "Nice! It seems that we have data from 1990 to 2019. Why \"seems\"? Well, we may have missing values..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4158ae-1368-4abf-9e25-d2fbb82a6df7",
   "metadata": {},
   "source": [
    "### Cleaning your data frame <a name=\"Cleaning-your-data-frame\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa593f-31e4-43ba-9c53-a3164733b1d7",
   "metadata": {},
   "source": [
    "Ideally, you would have perfect data and directly perform some statistical or machine learning analysis to uncover the mysterious relationships behind your data. Well, unfortunately, we don't live in a perfect world and your data will never be cleaned, except if someone already performed this fastidious task. In such case, be grateful, data scientists spend most of their time collecting and cleaning data.\n",
    "\n",
    "We will explore in this section how to clean our data frame. Unfortunately, there is no universal method: each dataset is different, and you will need to design the appropriate techniques depending on your objective and on how your data looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf15aa-4f25-49d8-a823-67ce48f5efca",
   "metadata": {},
   "source": [
    "#### Identifying NaN <a name=\"Identifying-NaN\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c80423-8712-4ea5-8b4b-bf7f0a2fbf13",
   "metadata": {},
   "source": [
    "The first step is to identify missing values. You have probably noticed something called `NaN` in our data frame. For example, for the GDP of Belgium in 1990:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df77eab-c3a6-45c1-bf0b-fd7a0bb0b791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[0, 'GDP [million Euro]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae38ebae-9711-40ce-a0be-6dbf80d14d2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "`NaN` stands for **Not a Number**, and it can be interpreted as a value that is undefined or unrepresentable. When you import csv data, the following values will be interpreted as `NaN`: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, ‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, ‘n/a’, ‘nan’, ‘null’."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e167c9b-88ad-415e-baf0-1681b854e2fc",
   "metadata": {},
   "source": [
    "We can check whether each value is a `NaN` or not using the `.isna()` method. `.isna()` will return `True` if the values are `NaN`, and `False` otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90bc378-0356-4477-a609-c074165956a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d88364-7ce7-4df9-920d-b08a9a7d8989",
   "metadata": {},
   "source": [
    "Ok, that's somewhat useful, but we are only human, we cannot check our 900x17=15300 observations to identify where we have `NaN`. Instead, let's use the power of pandas to pinpoint the location of our `NaN`. We will use the `.sum()` method, to sum over a column. Remember, `True` is associated with the value `1` and `False` with the value `0`, so when summing over a column of `True`/`False`, we are actually counting the number of `True` statement. In our case, we are counting the `Nan` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1180fc0a-054b-4147-9b50-125e805da658",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7159fb3e-7873-4cf8-8293-b986686eea74",
   "metadata": {},
   "source": [
    "Alright, we are lucky, it seems the problem is coming from the GDP data (and one population observation).\n",
    "\n",
    "We can extract the observations (rows) for which the GDP or the population is actually `NaN`. Remember what we did when we extracted data for Switzerland? We used a condition as indexing. We do the same here, this time our condition is checking the `NaN` values with `.isna()` but only for the GDP or the Population columns. The bitwise (element-wise) operator corresponding to or is `|`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a19e7-3fcd-4b6c-a991-6ad19fc353e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['GDP [million Euro]'].isna()) | (df['Population'].isna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2242a322-7d0c-4de9-aff5-5901f5c610b4",
   "metadata": {},
   "source": [
    "Ok we are making progress! We have now identified the rows containing `NaN`. If we explore a bit further, it seems that the missing observations are only for certain years. Let's confirm it by returning the years for which we have `NaN`. We just use the syntax `.Year` to extract the column (equivalent to `[\"Year\"]`), and then apply the `.unique()` method so that the given years only appear once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d039b80b-c8e4-4bfc-ba60-bec6e42f1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['GDP [million Euro]'].isna()) | (df['Population'].isna())].Year.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513be2a1-784e-4642-af01-265daeec6a2c",
   "metadata": {},
   "source": [
    "Great! Now we know that our missing values are only for years before 1994.\n",
    "\n",
    "We can also find which rows contain `NaN` applying the method `.index` to our data frame. Alternatively, let's create a list of the labels using list comprehensions. We can use the method `.iterrows()` to iterate over the labels and series of row values. In the code below, we also make use of the `.any()` function, which returns `True` if at least one item in our iterable is `True`, and `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7c245-0ae3-47ea-b324-a7d5a41c2c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_nan = [index for index, row in df.iterrows() if row.isna().any()]\n",
    "\n",
    "print(rows_with_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66975bc3-e351-47f9-9c3f-298832444f02",
   "metadata": {},
   "source": [
    "#### Droping NaN <a name=\"Droping-NaN\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f56031-a780-4442-b4b9-2b8e6d956861",
   "metadata": {},
   "source": [
    "We have previously identified the rows and columns that contain `NaN`. What do we do with these observations? Well, as often, it depends on what you want to achieve. In some context, `NaN` might not be an issue. Often, though, you will want to delete the observations containing `NaN`. How do we do that? Easy! We can use the `.dropna()` method. As the name indicates, it will drop the rows containing  `NaN`. \n",
    "\n",
    "Note that until now, when we operated on our data frame, we never modified its values. For instance, when we extracted a subset of our data frame - say the data for Switzerland - it did not modify our original data frame. We just returned a new object, and we did not store that object. It is the same with `.dropna()`. If we do not store the result, it will not affect our original data frame. Since when we will analyze the data we wish to work on the cleaned version of our data frame, we should store our result in a new data frame variable. \n",
    "\n",
    "As a good practice, you should <span style=\"color:dodgerblue\">store the modifications you make into a new data frame variable </span>, instead of just replacing the data frame you imported. Indeed, cleaning is an iterative process, and sometimes you might need to go back to a previous cleaning step, especially when you do a mistake..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69328ddc-7ccc-42fc-b465-0127e908953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.dropna()\n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458fbeef-2c4b-4baf-8101-45e33b8b8bb3",
   "metadata": {},
   "source": [
    "It worked! instead of 900 rows, we know have 786 rows and no more nasty `NaN`. We can confirm it by checking if we have `NaN` values with `.isna()` and then summing two times (one over the rows, the second over the columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feb4bb1-fb77-4a55-9750-44148d0d37f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e536ed1-ead7-4197-8101-1afe40cbd521",
   "metadata": {},
   "source": [
    "Great! Now we are ready data analysis! Wait a minute, not so fast! We dropped `NaN` values but by doing so created some imbalances in our data frame: some countries have more years than others. We can check by grouping our observations by country - we will perform such grouping operations a bit later.\n",
    "\n",
    "Although a slight imbalance might not be a big deal in some applications, let's suppose it is for us. What can we do to solve this issue? We have previously seen that we had `NaN` values only for years from 1990 to 1994. Easy then, let's drop all observations for years (strictly) before 1995. We can use the `.drop()` method. `.drop()` removes the rows corresponding to a specified label. Hence, we need to use the method `.index` to get the names of the labels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537fc467-8c91-4693-802f-b274516b91c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop(df_clean[df_clean.Year < 1995].index)\n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a5b646-5df9-4329-8d34-1ea67e78a3c9",
   "metadata": {},
   "source": [
    "Youhouuu! We know have a beautiful data frame without `NaN`. Now let's perform some data analysis! Wait a minute... We're not done with cleaning..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f757574-7bd6-445c-90b7-e84d3f465272",
   "metadata": {},
   "source": [
    "#### Dealing with errors and other missing values <a name=\"Dealing-with-errors-and-other-missing-values\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f9395e-cd1e-4a1b-ba5a-86e6fd7c43e2",
   "metadata": {},
   "source": [
    "Missing values can take different forms depending on the data you import. You might also have errors or \"absurd\" values. Dealing with those is another part of cleaning...\n",
    "\n",
    "Do you remember when we checked the data types of our columns, we got an `object` type for the emissions of arsenic (As), nickel (Ni), and chromium (Cr). Weird right, shouldn't we have numerical data like for the other emissions of pollutants? Well, yes, we should. So what is happening here? It happens that we have some strings in our columns instead of nice numbers. More precisely, the culprit strings are `'Not Available'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65de39b-a043-4c52-8045-8fb48e85539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.loc[238, 'Emissions_As [tonne]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac86e38-b89d-498b-a06f-643e8f2e6155",
   "metadata": {},
   "source": [
    "As we did for `NaN`, let's check for which variable we have such strings. We cannot rely on the `.isna()` method, since we are not dealing with `NaN`. Instead, we can use a simple condition, and then summing over our rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9545c63e-6e70-442d-b7f9-c2c5087c52c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_clean == 'Not Available').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6772fea1-7a53-4c4a-80cd-3062f51664b2",
   "metadata": {},
   "source": [
    "As suspected, the `'Not Available'` strings only appear for the emissions of arsenic (As), nickel (Ni), and chromium (Cr), thus explaining why these columns have the `object` type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dd35c5-204a-4582-b22d-52d46363ebe6",
   "metadata": {},
   "source": [
    "Let's continue our investigation by identifying the rows containing `'Not Available'`. To start with, we will only check for the emissions of arsenic column (`'Emissions_As [tonne]'`). Indeed, we can see above that we have 10 observations containing `'Not Available'` for the emissions of arsenic, nickel, and chromium. It feels likely that the missing data for these pollutants are for the same country-years..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48a9706-2d6b-464d-89e8-629995ee6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[df_clean['Emissions_As [tonne]'] == 'Not Available']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee928dc6-90ce-4a9a-b03f-ddc774cd2cb8",
   "metadata": {},
   "source": [
    "Bingo! We got 10 rows, and if we look at the arsenic, nickel, and chromium columns, all the values are not available. \n",
    "\n",
    "Ok, so what do we do now. We could drop the country-years with missing values as we did before with `NaN`, or all the observations for Switzerland and Luxembourg, or the columns with the emissions of As, Ni, and Cr. However, in our context, it feels a bit extreme. Remember, what you do depends on your ultimate goal. And **data science is not only about good programming techniques, it also about domain-expertise**. I know, from previous studies and ex-post analysis, that the emissions of arsenic, nickel, and chromium are very small and thus represent a small part of the external costs of pollution, at least in other countries. There is no reason to suspect a different pattern for Switzerland and Luxembourg, especially since these countries emit relatively less pollutants than others (for the pollutants for which we have data). Thus, instead of deleting observations, we will assume that the emissions are equal to zero. \n",
    "\n",
    "The influence of such assumptions on our result should be negligible. Still, you should always <span style=\"color:dodgerblue\">document your assumptions </span> and you can even do ex-post <span style=\"color:dodgerblue\">sensitivity analysis </span>, by trying different values and see how your results are affected. \n",
    "\n",
    "So we decided to assume that the emissions were zero. How do we implement that? It's actually not difficult since we can use the `.replace()` method. In `.replace()`, the first argument is the value we wish to replace, the second one is the value we replace with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137447be-9fec-49de-a831-907aa452f540",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.replace('Not Available', 0)\n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861f36f-a7a2-43c0-825d-491fbca6e11b",
   "metadata": {},
   "source": [
    "See, we still have 750 rows, so we did not drop any observation. Let's check that the `'Not Available'` string disappear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102001bb-16d1-418f-b901-4d6c10dd5744",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_clean == 'Not Available').sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f428a-5583-46b1-89dc-7a3d4a1166b5",
   "metadata": {},
   "source": [
    "Yihaaa! A well-done job! Let's perform one last cleaning operation. We will modify the type of our columns. Indeed, the `object` type might be inconvenient when we will perform numerical operations. We will use the `to_numeric` pandas function and the `.apply()` method, which allows us to apply our function along one axis of our dataframe, in this case to each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e0e8aa-f0ac-47f0-904a-a1b9139d8a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_hm_obj = ['Emissions_As [tonne]', 'Emissions_Ni [tonne]', 'Emissions_Cr [tonne]']\n",
    "\n",
    "df_clean[list_hm_obj] = df_clean[list_hm_obj].apply(pd.to_numeric)\n",
    "\n",
    "df_clean.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca650c4-18eb-4425-8181-5a4b41401e57",
   "metadata": {},
   "source": [
    "This time, we are actually done with cleaning. Not so painful, no? Huge props to pandas who made cleaning an actually bearable experience.\n",
    "\n",
    "You might wonder, how do I know we are done with cleaning? Well, here, it's because I know our data set, since I prepared it! In practice, you need to further explore your data, for instance using Exploratory Data Analysis, which we will see in a future lesson. It might also happen that you only realize something is wrong with the data once you started to analyze it. Although it might be very annoying, remember that **data science projects are an iterative process**, and you always go back and forth.\n",
    "\n",
    "Before moving on to some data manipulation, we will reset the labels of our rows - as you can see above, the rows numbers were the ones from our original data frame. We can simply do that using the method `.reset_index`. The argument `drop=True` drops the previous labels, while `drop=False` (default value) will insert the previous labels in a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6bce39-1ca2-4130-b456-aa7c72461de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.reset_index(drop=True)\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f655b491-0b79-4c23-8b67-7eadf5f81a78",
   "metadata": {},
   "source": [
    "### Merging data frames <a name=\"Merging-data-frames\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e80cc6-b845-44f7-8784-2f67b96ba804",
   "metadata": {},
   "source": [
    "The first step when we discovered Pandas was to import data. We have imported the dataset we are currently using, which contains the GDP, population, and emissions of pollutants for European countries since 1990. But do you remember we also imported a second dataset? Well, we did not import that other dataset just for fun, we will actually need it for the GrDP analysis! No worries though, the dataset is already clean...\n",
    "\n",
    "Let's first have a look at what it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c391c178-196e-476d-ae49-c72c3a027198",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbecdac5-0c5a-44a7-95c1-35785804ce6c",
   "metadata": {},
   "source": [
    "We have, for the same pollutants than before, their unit damage cost. The damage costs are the costs and expenses that are or would be incurred to prevent, control or abate the environmental harm caused by pollution. We notice that the costs vary per country for air pollutants (NO$_x$, PM$_{2.5}$, PM$_{10}$, SO$_x$, NMVOC, NH$_3$). Indeed, these pollutants induce a local pollution: they are transported in the air over relatively short distance before being inhaled by humans and contaminating soil, water, and ecosystems (at which point they could be ingested by humans). By contrast, greenhouse gases (GHG) are responsible for climate change, which is a global pollution: it does not matter where the emissions occur, all GHG emitted contribute to global warming. As for heavy metals (Pb, Cd, Hg, As, Ni, Cr), they induce a local pollution, but the available data was only for an average European damage cost.\n",
    "\n",
    "We note that the costs are independent of years. Indeed, all the monetary data (e.g., GDP) are expressed in 2019 euros.\n",
    "\n",
    "GrDP is defined as GDP minus the external costs resulting of human activities. In its current version, the indicator only includes the external costs for which the impacts are known, measured, and priced; namely GHG, air pollutants, and heavy metals. \n",
    "\n",
    "To compute GrDP, we will need to multiply the emissions data - contained in our data frame `df_clean` - by our unit cost data - included in the data frame `df_data`. It will be more convenient to operate on a single data frame. Thus, we need to merge our two data frames.\n",
    "\n",
    "This seems complex though: not only do the columns differ, but also the rows since the emissions vary for each year and the costs do not. No worries, Pandas is here to come to the rescue! There are several methods and functions to combine two data frames:\n",
    "\n",
    "- `.concat()` combines data frame along an axis, either across rows or columns. \n",
    "- `.join()` combines data on key column or an index.\n",
    "- `.merge()` combines data on common columns or indices.\n",
    "\n",
    "To better understand how these methods work, I strongly recommend reading the associated [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html). You can also check this [Real Python tutorial](https://realpython.com/pandas-merge-join-and-concat/#pandas-concat-combining-data-across-rows-or-columns).\n",
    "\n",
    "In our case, we want to combine our two data frames such that, for each country-year observation, we have the variables GDP, population, emissions of each pollutants, and costs of each pollutants. Our two data frames have one column in common, the country. Moreover, we want to keep all the observations from the `df_clean` data frame while the costs for a given country should be the same for each year, i.e., we need to 'copy' the information of the `df_cost` data frame. We can achieve this using the `.merge()` function. \n",
    "\n",
    "The values of the column(s) on which we merge our data frames are called the keys. If you dive a bit into the `.merge()` documentation, you will notice that you have different options to deal with cases where the keys differ (e.g., suppose that we have different countries in our two data frames):\n",
    "- `inner` only keeps the rows with common keys, i.e., the intersection;\n",
    "- `outer` keeps all rows, i.e, the union;\n",
    "- `left` keeps all rows from the left (first) data frame, and disregards the ones from the right (second) data frame when the keys do not exist in the left data frame;\n",
    "- `right` keeps all rows from the right data frame, and disregards the ones from the left data frame when the keys do not exist in the right data frame;\n",
    "- `cross` creates the cartesian product of rows of both frames, i.e., it creates rows with all possible combination of keys.\n",
    "You can specify the method with `how`.\n",
    "\n",
    "Fortunately, in our case, we do not need to worry about the method since the countries are the same.\n",
    "\n",
    "Combining data frames requires some practice, so try it by yourself, always check the result, and try again if you did not obtain what you expected! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e0e480-4bbc-4b22-88dd-d63293ea21ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grdp = pd.merge(df_clean, df_cost, how=\"left\", on=\"Country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c65eb31-eba4-4bfe-965d-a7aa3cfd7a91",
   "metadata": {},
   "source": [
    "Note that in the code above, we did not need to specify `on` which columns we should merge since the function automatically select all the common columns. Similarly, we did not need to specify `how` to merge since our keys are the same.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fe546a-6285-47aa-a8aa-dddbc5789886",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grdp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea677d1-1d80-4089-abfc-4874c3fe4b7d",
   "metadata": {},
   "source": [
    "### Manipulating your data <a name=\"Manipulating-your-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0add131d-cb6f-4623-b002-8062ad81fe1a",
   "metadata": {},
   "source": [
    "#### Operating on columns <a name=\"Operating-on-columns\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1565f4f7-02f6-4a3e-9d33-6050c11afcc1",
   "metadata": {},
   "source": [
    "Let's start with some simple operations. We have data for GDP and for population. To compare countries, it is best to use per capita indicators. Hence, let's compute the GDP per capita. We want to divide the elements of the GDP column by the elements of the population column. Should we use so kind of comprehension techniques as we did with lists? No! In pandas, when we want to divide one column by another element-by-element, we can simply divide the two columns. Also, similar to what we did with dictionaries, we can easily create a new column in our data frame indexing on a new name: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63538cc-e42b-446e-a05b-ad83bbcaeffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grdp[\"GDP per capita\"] = df_grdp[\"GDP [million Euro]\"]/df_grdp[\"Population\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52e1cc-efc4-474d-90d9-2af51fc1a007",
   "metadata": {},
   "source": [
    "Note that the same syntax can be used for any element-by-element operations including addition, subtraction, or multiplication. Very convenient isn't it?\n",
    "\n",
    "Let's visualize the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99890cb-b477-4688-8024-8e5af2ef5657",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grdp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ab7cae-0570-4bfc-b45b-a983f199f496",
   "metadata": {},
   "source": [
    "Looks like it worked. However, because our GDP data was in million euros, our GDP per capita is in million euros per capita. Countries do not produce this much, so the outputs are very small... Let's modify the unit to euros per capita by multiplying the column by 1'000'000. As before, we do not need some kind of comprehension, we can directly multiply the column by our scalar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7916cb3c-4863-4726-90b0-88d6b6ac5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grdp['GDP per capita'] = df_grdp['GDP per capita']*1000000\n",
    "\n",
    "# Here is an alternative:\n",
    "# df_data.loc[:, 'GDP per capita'] *=1000000  \n",
    "\n",
    "df_grdp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb77ce3-9981-4000-a9de-c2174ff9e930",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2be61b-3805-4681-87f9-d71660783d08",
   "metadata": {},
   "source": [
    "#### Functions and data frame <a name=\"Functions-and-data-frame\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8c7cc8-055e-4f59-a7cc-b937f25cb2f5",
   "metadata": {},
   "source": [
    "We will now calculate the Green Domestic Product. First let's compute the external costs of each pollutant. We need to multiply each emission column with the associated unit cost column, while paying attention that the unit is matching. For instance, for greenhouse gases, we can calculate the external costs in million euros with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707690a3-2534-46a5-9217-41700c133ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grdp[\"External cost GHG\"] = (df_grdp[\"Emissions_GHG [thousand tonnes CO2eq]\"]\n",
    "                                *df_grdp[\"Cost_GHG [Euro per tonnes CO2eq]\"]/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d585c-f07f-4e23-b49d-1caf20bc38f4",
   "metadata": {},
   "source": [
    "We could proceed similarly for all pollutants... But that would be redundant. Surely we can do better? Yes we can. We will define some functions to repeat the operation!\n",
    "\n",
    "Let's proceed by group of pollutants, first with the air pollutants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1844e0e0-97fd-4fdf-a279-61b57e1e2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def external_cost(pol):\n",
    "    \"\"\"This function computes the external cost of air pollutants, \n",
    "    and stores the results in a new column of our data frame.\n",
    "    It takes one argument: pol refs to a pollutant and should be a string\"\"\"\n",
    "    \n",
    "    df_grdp['External cost {p}'.format(p=pol)] = (df_grdp['Emissions_{p} [tonne]'.format(p=pol)]\n",
    "                                             *df_grdp['Cost_{p} [Euro per tonne]'.format(p=pol)]\n",
    "                                             /1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fe6a0d-de3d-4d03-9669-961845f8ca2f",
   "metadata": {},
   "source": [
    "Can you understand this function? We made use of the repeated patterns in our variables names and of the string `.format` method. Recall that `.format` allows to insert values into a string. Here, the idea is to insert the name of our pollutant. The rest of the code performs the exact same operation as we did above with GHG: it multiplies the emission of a given pollutant by the associated unit cost and stores the results in a new column. We divide by 1'000'000 to express the result in million euros. \n",
    "\n",
    "Ok, now we can apply our function to our air pollutants. One way is to loop over our list of air pollutants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba061fee-cf47-4495-b0ec-aee4cba032d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_pollutants = ('NOx', 'PM2.5', 'PM10', 'SOx', 'NMVOC', 'NH3') # tuple of air pollutant\n",
    "\n",
    "for pol in air_pollutants:\n",
    "    external_cost(pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34447b9-0bc5-463e-936b-fd44d7a0bf7f",
   "metadata": {},
   "source": [
    "Alright, let's do the same with the heavy metals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed7277-bdec-4d2d-8045-313268114436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def external_cost_hm(pol):\n",
    "    \"\"\"This function computes the external cost of heavy metal, \n",
    "    and stores the results in a new column of our data frame.\n",
    "    It takes one argument: pol refs to a pollutant and should be a string\"\"\"\n",
    "    \n",
    "    df_grdp['External cost {p}'.format(p=pol)] = (df_grdp['Emissions_{p} [tonne]'.format(p=pol)]\n",
    "                                             *df_grdp['Cost_{p} [Euro per kg]'.format(p=pol)]\n",
    "                                             /1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d058cdb-3061-490e-bf5b-81009d01ae45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heavy_metals = ('Pb', 'Cd', 'Hg', 'As', 'Ni', 'Cr') # tuple of air pollutant\n",
    "\n",
    "for pol in heavy_metals:\n",
    "    external_cost_hm(pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7411fdf-90f9-472b-9e1e-a09941897b00",
   "metadata": {},
   "source": [
    "As always, let's visualize the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d4b61-9554-40e1-81f0-afeb468697e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grdp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70456c-db74-4715-abfe-7d785cc25509",
   "metadata": {},
   "source": [
    "And we can check that we did not miss one pollutant by returning a list of the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c916fa8-6dca-474f-8d70-a5e9e59fe793",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_grdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a52d495-694d-4ea3-a13a-e167a22ea497",
   "metadata": {},
   "source": [
    "Looks good to me! We're almost done!\n",
    "\n",
    "Let's compute the total external cost by summing each pollutant's external cost. We use the `.sum` method, where `axis=1` means that we are summing over columns (we could also sum over rows):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d043967-0469-4d27-9ee1-a7173a903cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grdp['Total external cost'] = df_grdp.loc[:,'External cost GHG':'External cost Cr'].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0824b6-037c-4f16-a987-407aaca909ff",
   "metadata": {},
   "source": [
    "#### Calculating GrDP <a name=\"Calculating-GrDP\"></a>\n",
    "\n",
    "Finally, we can compute the Green Domestic Product. We also compute a few additional indicators, such as the GrDP per capita, and the share of external cost with respect to GDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0d2b1-d815-4858-a091-7778406df7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GrDP = GDP - total external cost\n",
    "df_grdp['GrDP [million Euro]'] = df_grdp['GDP [million Euro]'] - df_grdp['Total external cost']\n",
    "\n",
    "# GrDP per capita = GrDP / population\n",
    "df_grdp['GrDP per capita [Euro]'] = df_grdp['GrDP [million Euro]']*1000000/df_grdp['Population']\n",
    "\n",
    "# Share of external cost w.r.t. GDP\n",
    "df_grdp['Share external cost [%]'] = df_grdp['Total external cost']*100/df_grdp['GDP [million Euro]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987e592d-2d71-46c7-a1e6-9b2febf9607b",
   "metadata": {},
   "source": [
    "Let's visualize the result.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc256c7c-7ba0-4c61-98e5-266ea2a94e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grdp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d03b845-d5e0-4a84-9a84-8208103c4673",
   "metadata": {},
   "source": [
    "Here you go! We have computed the GrDP and related indicators for European countries between 1995 and 2019! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f390051f-615c-4e74-95eb-b37323d7e750",
   "metadata": {},
   "source": [
    "### Exporting data frame <a name=\"Exporting-data-frame\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53200d61-c00b-4000-b9f6-2248a555c3b0",
   "metadata": {},
   "source": [
    "We have created this super cool data frame. We will now save our data into a CSV file to be able to reuse it. We use the `.to_csv` method to export to CSV. We specify a name for our file, while the kwarg `index = False` ask Pandas not to explicitly write the row labels to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdfdd25-9b94-4f40-992e-a15927209e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grdp.to_csv('GrDP_1995-2019.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45a9da-4581-41c4-82fd-591d5f582965",
   "metadata": {},
   "source": [
    "Our file is saved in the active directory. In Colab, click on the folder icon (left of the screen), and you can then download your file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
