{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0e1e41-0543-4373-adfd-57e155d8a2d0",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/edoardochiarotti/class_datascience/blob/main/2023/02_Data-Cleaning/Resources/02_Data-cleaning-Key-concepts.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9980e5b5-4dd8-44b4-9af3-8a72da9a4494",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Statistics and Data Science: Data cleaning - Key concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bf30d7-06c0-4284-b735-3810197cafa4",
   "metadata": {},
   "source": [
    "<img src='https://miro.medium.com/max/800/1*Xhm9c9qDfXa3ZCQjiOvm_w.jpeg' width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96027b29-1c6b-47ca-acee-10e8b165082f",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "This notebook introduces the concepts of data, data preprocessing, and data cleaning, including practical advices on how to perform these operations.\n",
    "\n",
    "- [The job of Data Scientist](#data-scientist)\n",
    "- [Where do (companies) get data?](#data-company)\n",
    "- [Data format and type](#data-format)\n",
    "    - [CSV file](#data-csv)\n",
    "    - [JSON file](#data-json)\n",
    "    - [XML file](#data-xml)\n",
    "- [Data preprocessing tasks](#data-preprocessing)\n",
    "- [Data cleaning](#data-cleaning)\n",
    "    - [Missing data](#data-missing)\n",
    "    - [Noise, erroneous values, and outliers](#data-outliers)\n",
    "    - [Duplicate data and multiple representations](#data-duplicate)\n",
    "- [Data transformation](#data-transformation)\n",
    "- [When to stop data preprocessing?](#data-stop)\n",
    "- [Key points](#data-key-points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f78dd1-7d47-4d15-a9f6-83fe74e9728b",
   "metadata": {},
   "source": [
    "## The job of Data Scientist <a name=\"data-scientist\"></a>\n",
    "\n",
    "The job of the Data Scientist is add value to the data by consolidating, cleaning, enriching and building predictive models using the data.\n",
    "\n",
    "In practise, though:\n",
    "\n",
    ">  Data Scientists spend 80% of their time on data collecting and data cleaning operations. \n",
    ">\n",
    "> The remaining 20% , they complain about cleaning data! \n",
    "\n",
    "It's not even a joke, data preparation accounts for about 80% of the work of data scientists:\n",
    "\n",
    "<img src='https://www.researchgate.net/profile/Houda-Sarih/publication/335577003/figure/fig2/AS:801487160631296@1568100838487/What-data-scientists-spend-the-most-time-doing-7.ppm' width=\"500\">\n",
    "\n",
    "Source: Gil Press, [Cleaning Big Data: Most Time-Consuming, Least Enjoyable Data Science Task, Survey Says](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/), Forbes, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03df0972-a95d-45bd-ac05-aac50928ce4a",
   "metadata": {},
   "source": [
    "## Where do (companies) get data? <a name=\"data-company\"></a>\n",
    "\n",
    "Internal company data:\n",
    "- Transactional data: e.g., what customers bought, what companies bought from suppliers\n",
    "- Behavioral data: e.g., what customers browsed on the website\n",
    "- Process data: e.g., sensors\n",
    "\n",
    "External data:\n",
    "- Public data: open-source data\n",
    "    - [ADEME](https://www.ademe.fr/en/our-missions/data/) (environmental impacts), [BOAVIZTA](https://www.boavizta.org/en) (environmental impact of digital technology) \n",
    "- Premium data: purchased data\n",
    "    - [ECOINVENT](https://ecoinvent.org/) (life cycle inventory database)\n",
    "\n",
    "External data can be retrieved by bulk download, via [API](https://en.wikipedia.org/wiki/API), [web scraping](https://en.wikipedia.org/wiki/Web_scraping), etc.\n",
    "\n",
    "The system that assembles the different data available in the entreprise is called the **date warehouse**. A data warehouse supports the management's decision making process.\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/8/8d/Data_warehouse_architecture.jpg?20160117164449' width=\"400\">\n",
    "\n",
    "Source: [Data Warehouse architecture](https://commons.wikimedia.org/wiki/File:Data_warehouse_architecture.jpg), by Soha jamil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c40458-742f-4a10-9eb7-2275309fc17c",
   "metadata": {},
   "source": [
    "## Data format and type <a name=\"data-format\"></a>\n",
    "\n",
    "Data can be structured (e.g., database tables), semi-structured (e.g., [JSON](https://en.wikipedia.org/wiki/JSON), [XML](https://en.wikipedia.org/wiki/XML)), or unstructured (e.g., text, videos, images). Before any analytics model can be built, unstructured data has to be converted into a structured data.\n",
    "\n",
    "<img src='https://i.postimg.cc/BQcLPL0f/Data-type.png' width=\"700\">\n",
    "\n",
    "- Numerical/Quantitative data:\n",
    "    - Continuous: data can be measured and take an infinite number of values, e.g., temperature, GHG emissions, GDP\n",
    "    - Discrete: data takes certain values, e.g., number of environmental policies \n",
    "- Categorical data:\n",
    "    - Ordinal: distinct and can be ordered, e.g., credit score can be {low, medium, high}, education level {high-school, college, graduate school}\n",
    "    - Nominal: categories cannot be ordered, e.g., gender, zip code, land categories {urban areas, forest, agriculture areas}\n",
    "\n",
    "You may ask, why do we care? \n",
    "\n",
    "Well, for numerical data, the type of data will affect your research question and the choice of model: e.g., [linear regression](https://en.wikipedia.org/wiki/Linear_regression) for continuous data; [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) / [probit model](https://en.wikipedia.org/wiki/Probit_model) for discrete data.\n",
    "\n",
    "As for categorical variables, we will need to transform them into numerical to use them in our analysis:\n",
    "- For ordinal data, we can use label encoding: convert each value in a column to a number\n",
    "- But for nominal data, since values cannot be ordered, one-hot encoding would be preferred: each category value is converted into a new column and assigned a 1 or 0 (true/false) value to the column\n",
    "\n",
    "Further reading: [Categorical encoding using Label-Encoding and One-Hot-Encoder](https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd)\n",
    "\n",
    "**You should convert your variables into structured data with the correct format and type before doing any further processing**\n",
    "\n",
    "Below we present a few common data format that you will probably encounter in your data science projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eef8e9-a317-4811-9e68-4e00a93d006d",
   "metadata": {},
   "source": [
    "### CSV file <a name=\"data-csv\"></a>\n",
    "\n",
    "[Comma-separated values (CSV)](https://en.wikipedia.org/wiki/Comma-separated_values) is a common data exchange format. It is text file format that uses commas to separate values. \n",
    "\n",
    "<img src='https://i.postimg.cc/qqpQVtVc/CSV-file.png' width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26a0c8f-0e75-4488-b12b-605733dc73f8",
   "metadata": {},
   "source": [
    "### JSON file <a name=\"data-json\"></a>\n",
    "\n",
    "[JavaScript Object Notation (JSON)](https://en.wikipedia.org/wiki/JSON) is a text format for storing and transporting data. It is \"self-describing\" and easy to understand.\n",
    "\n",
    "<img src='https://i.postimg.cc/JhGkr9by/JSON-file.png' width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbfa9f9-c454-4449-850c-5a0c4f85c487",
   "metadata": {},
   "source": [
    "### XML file <a name=\"data-xml\"></a>\n",
    "\n",
    "[Extensible Markup Language (XML)](https://en.wikipedia.org/wiki/XML) is a markup language much like HTML. It was designed to store and transport data and is is \"self-descriptive\".\n",
    "\n",
    "<img src='https://blog.hubspot.com/hs-fs/hubfs/xml%20file%20example%20storing%20names%20and%20test%20scores%20of%20students-1.png?width=650&name=xml%20file%20example%20storing%20names%20and%20test%20scores%20of%20students-1.png' width=\"500\">\n",
    "\n",
    "Source: Valentyn N Sichkar, [XML_files_in_Python](https://github.com/sichkar-valentyn/XML_files_in_Python/blob/master/example.xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b56ca9-20c7-486b-acd3-d81b753a80bb",
   "metadata": {},
   "source": [
    "## Data preprocessing tasks <a name=\"data-preprocessing\"></a>\n",
    "\n",
    "- ***Data integration/consolidation*** merges data from multiple sources into a coherent data store\n",
    "- ***Data cleaning*** can be applied to detect missing data, remove noise, and correct inconsistencies in data.\n",
    "- ***Data transformations*** (e.g., normalization) may be applied, where data are scaled to fall within a smaller range like 0.0 to 1.0. This can improve the accuracy and efficiency of algorithms.\n",
    "- ***Data reduction*** can reduce data size by, for instance, aggregating, eliminating redundant features, or clustering.\n",
    "\n",
    "<img src='https://www.researchgate.net/profile/Martin-Andreoni/publication/327183263/figure/fig21/AS:662772148887552@1535028602008/Preprocessing-steps-composed-of-Data-Consolidation-Data-Cleaning-Data-Transformation.png' width=\"600\">\n",
    "\n",
    "Source: Martin Andreoni, [A Monitoring and Threat Detection System Using Stream Processing as a Virtual Function for Big Data](https://www.researchgate.net/publication/327183263_A_Monitoring_and_Threat_Detection_System_Using_Stream_Processing_as_a_Virtual_Function_for_Big_Data) \n",
    "\n",
    "Note that this process is actually not linear. In practice, you will collect some data and start cleaning and transforming them; then start your exploration and analysis, and realize you may need more data and collect some new data from another source, merge with the previous ones, clean, transform, reduce; and do it again...\n",
    "\n",
    "**Data science is an iterative process!** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e75e51-dc72-4a26-96e2-d532227923fa",
   "metadata": {},
   "source": [
    "## Data cleaning <a name=\"data-cleaning\"></a>\n",
    "\n",
    "<img src='https://i.imgflip.com/80cbhh.jpg' width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493598fb-7040-4b12-833a-c49922bbaeca",
   "metadata": {},
   "source": [
    "### Missing data <a name=\"data-missing\"></a>\n",
    "\n",
    "Data can be missing for many reasons: someone may decline to respond to a survey, there may be sensor failure, some countries may report some indicators that others do not, etc.\n",
    "\n",
    "How to handle missing values? First, you need to **understand if the values are missing at random or not**:\n",
    "- Missing Completely At Random (MCAR): all the variables and observations have the same probability of being missing\n",
    "- Missing At Random (MAR): the probability of the value being missing is related to the value of the variable or other variables in the dataset\n",
    "- Missing Not At Random (MNAR): the probability of being missing is completely different for different values of the same variable\n",
    "\n",
    "Then, there are several strategies depending on the context:\n",
    "\n",
    "|Action|Strategy | Pros| Cons|\n",
    "|--|--|--|--|\n",
    "|Deletion|Delete observations that have one or more missing values  |Straightforward and simple <br /> Beneficial when observation has little importance | Information loss <br /> Introduction of bias <br /> Not appropriate when data is not MCAR |\n",
    "||Discard variables with large proportion of observations missing |Straightforward and simple <br /> Beneficial when variable has little importance| Information loss <br /> Not appropriate when variable is necessary for analysis |\n",
    "|Ignore|Ignore missing values in the analysis|Straightforward and simple  <br /> No information loss compared to deletion |Results of analysis lack comparability (regression using different number of observations) <br /> Not appropriate when data is not MCAR |\n",
    "|Imputation|Replace with mean/median/mode|Simple and easy to implement <br />Rely on existing data, no additional data needed <br /> Good estimate for normally-distributed data (mean) and skewed data (median)|Sensitive to outliers <br /> Does not make use of data characteristics <br />  Not appropriate when data is not MCAR |\n",
    "||Replace with adjacent value or linear interpolation|Simple and easy to implement <br />Rely on existing data, no additional data needed <br /> Good estimate of the missing values|Not applicable for time series with seasonality <br />  Not appropriate when data is MNAR|\n",
    "||Random Sample Imputation: draw values from distribution|Preserves original data distribution |Can infuse noise in data, leading to incorrect statistical conclusions |\n",
    "||Multiple Imputation: predict missing values using other variables|Flexible and powerful method |Difficult to implement <br /> Computationally-expensive <br />Not appropriate when data is MNAR|\n",
    "\n",
    "Further reading: [The prevention and handling of the missing data](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3668100/), by Hyun Kang\n",
    "\n",
    "Deleting observations raise questions on how to select your sample. As an advice, your should **favor smaller coverage and balanced dataset**:\n",
    "- Balanced dataset limits the risk of \"sample\" bias. For instance, a typical situation is that you have a shorter coverage for developing countries. Hence, in the most recent period, the sample is changing. Those change might be caused by some time varying factors but also by the sample changing over time. It is essential to avoid this.\n",
    "- Balanced dataset may increase the performance of some algorithms, e.g., for classification tasks\n",
    "\n",
    "Exploratory Data Analysis helps to spot heterogeneity and how balanced is the dataset. We'll cover it next week!\n",
    "\n",
    "Finally, remember, when dealing missing values:\n",
    "- **Rely on your domain-knowledge!**\n",
    "- **Carefully justify and document all your assumptions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368e04b7-3b93-411a-bb25-7dfb6d84d8f3",
   "metadata": {},
   "source": [
    "### Noise, erroneous values, and outliers <a name=\"data-outliers\"></a>\n",
    "\n",
    "<img src='https://miro.medium.com/v2/format:webp/0*ljrhkbPkmwXXjo5J.jpeg' width=\"250\">\n",
    "\n",
    "**Noise** refers to modification of *original* values because of errors in the data collection, sensor issues, data conversion issues, \"fat-finger\" errors, etc.\n",
    "\n",
    "**Outliers** are the observations in a dataset that deviate significantly from the rest of the data. **Outliers are not necessarily erroneous values!** They may indicate skewed distribution. Hence, **do not just throw away outliers; first investigate what cause them**.\n",
    "\n",
    "How to detect erroneous values and outliers?\n",
    "- **Rely on your domain knowledge** \n",
    "    - e.g., energy consumption or GHG emissions cannot be negative... Actually GHG emissions from land use, land use change, and forestry (LULUCF) can be negative - see [LULUCF emissions in Europe](https://www.eea.europa.eu/ims/greenhouse-gas-emissions-from-land)\n",
    "- **Use visualization techniques**\n",
    "    - e.g., box plot, scatter plot, histogram, and cluster analysis can help detecting outliers\n",
    "- **Use descriptive statistics**\n",
    "    - e.g., min/max, [Z-score](https://medium.com/clarusway/z-score-and-how-its-used-to-determine-an-outlier-642110f3b482)\n",
    "\n",
    "How to deal with outliers and erroneous values?\n",
    "\n",
    "|Action|When?|\n",
    "|--|--|\n",
    "|Delete the values| When you know that the value is wrong|\n",
    "|Change the values| When you know the reason for the outliers and the actual value | \n",
    "|[Data transformation](#data-transformation)| When dealing with highly skewed data (e.g., log-transform)|\n",
    "|Use different analysis methods| Some statistical tests are not much impacted by outliers, e.g., median (vs mean)|\n",
    "|Value the outliers| Outliers can provide valuable clues about a process when there is a valid reason for them|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5670aff-d1b9-4382-9312-14b1c907c95e",
   "metadata": {},
   "source": [
    "### Duplicate data and multiple representations <a name=\"data-duplicate\"></a>\n",
    "\n",
    "<img src='https://i.kym-cdn.com/entries/icons/original/000/023/397/C-658VsXoAo3ovC.jpg' width=\"300\">\n",
    "\n",
    "- Multiple representations occur when data is not standardized: the same value can appear as different values (e.g., Zurich, Zuerich, ZÃ¼rich). Multiple representations are often due to accents, diffent location names, time format, currencies, etc. In such case we need to standardise our data. As a side effect, it can introduce duplicates in our data set\n",
    "    - **Be careful to identify all multiple representations**\n",
    "- Duplicate data can occur due to data standardisation or, for instance, when someone fills in a survey several times. In such case, we should remove deplicates, a process called deduplication\n",
    "    - **Be careful to drop actual duplicates, and not duplicates *look-alike***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9ab28c-7432-4e8e-8b15-c8bc259df2ae",
   "metadata": {},
   "source": [
    "## Data transformation <a name=\"data-transformation\"></a>\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:720/format:webp/1*95O9zs1Qn30Y7xKZZcmf6g.jpeg' width=\"300\">\n",
    "\n",
    "Data transformation is useful, or even needed, to:\n",
    "- Convert categorical data to numeric\n",
    "    - e.g., say you want to predict whether water samples are safe or not, and your variable is categorical {safe, non-potable}, then you should convert the categories to {1,0} to perform your analysis\n",
    "- Convert a non-linear relationship to linear\n",
    "    - e.g., [wind power](https://en.wikipedia.org/wiki/Wind_power) is proportional to the cube of the wind speed \n",
    "- Compare results\n",
    "    - e.g., to compare waste generation across countries, you need values per capita\n",
    "- Enhance the performance of some algorithms\n",
    "    - normalize your features/variables, e.g., between 0 and 1, can help the accuracy of Machine Learning algorithms\n",
    "\n",
    "Some typical transformation\n",
    "- **Discretization**: convert continuous to discrete variables\n",
    "    - Can help when the relationship is not linear\n",
    "    - Sometimes, the algorithms we want to use are only defined for a certain type of data (e.g., for classification)\n",
    "    - Discretization implies losing some information and involves making choices that may effect the performance: e.g., How many bins? Where do we place the cutoffs?\n",
    "    - Do not repeatedly try different cuts of a continuous variable to obtain a statistically significant result! Design categories that you can justify.\n",
    "- **Log-transform**: $log(x)$\n",
    "    - Can be used:\n",
    "        - When the relationship between two variables is not linear, and we want to use a linear regression model. See for instance [Econometrics and the Log-Log Model](https://www.dummies.com/article/business-careers-money/business/economics/econometrics-and-the-log-log-model-156386/). \n",
    "        - When we want to interpret our coefficient with percentage change or elasticity \n",
    "        - When the distribution is highly positively skewed (*rule of thumb skewness > 3*) in the context of linear regression\n",
    "    - However, be careful: \n",
    "        - log-transformation **doesn't necessarily makes the distribution more symmetric**, **doesn't necessarily reduces the variability of data**, and applying a t-test on log-transformed data is not equivalent to the un-transformed test nor to a non-parametric test. Read [Log-transformation and its implications for data analysis](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4120293/) (Feng C. et al., 2014) to learn more\n",
    "        - log-transformation **changes the interpretation** of regression coefficients. Here is a full guide on [How to interpret Linear Regression Coefficients](https://towardsdatascience.com/how-to-interpret-linear-regression-coefficients-8f4450e38001), by Quentin. \n",
    "        \n",
    "        \n",
    "**Do not try all possible transformation until you obtain a statistically significant result!** You should be able to **justify and document your assumptions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b48bb-d6a9-4463-9831-ad7dbf01ca8b",
   "metadata": {},
   "source": [
    "## When to stop data preprocessing? <a name=\"data-stop\"></a>\n",
    "\n",
    "To know when to stop data preprocessing, i.e., the **readiness of your data, ask yourself**:\n",
    "\n",
    "|Metrics| Question|\n",
    "|--|--|\n",
    "|Data source reliability| Do we have right confidence in the source?| \n",
    "|Data content accuracy| Do we have the right data?|\n",
    "|Data accessibility| Can we easily get to the data when we need it?|\n",
    "|Data security and privacy| Accessible only to those with authority and need to access it?|\n",
    "|Data consistency | Is data consistent across tables?|\n",
    "|Data validity | Within the expected range of values?|\n",
    "|Data timeliness | Is it up-to-date?|\n",
    "|Data granularity | Is it at the level of detail required?|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f427637-314c-486b-9f74-38d702eb577f",
   "metadata": {},
   "source": [
    "## Key points <a name=\"data-key-points\"></a>\n",
    "\n",
    "- Data Science is an **iterative process**\n",
    "- You should convert your variables into structured data with the **correct format and type**\n",
    "- **Strategies to deal with missing values depend on the context**. Understand if values are missing at random or not\n",
    "- **Investigate what cause outliers**. Do not just throw them away blindly\n",
    "- Do not try all possible transformation until you obtain a statistically significant result!\n",
    "- Rely on your **domain-knowledge!**\n",
    "- Carefully justify and **document all your assumptions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de9a66b-b2a0-4ab6-a44f-a30194c56e5a",
   "metadata": {},
   "source": [
    "Finally, remember, data science is fun...\n",
    "\n",
    "<img src='https://i.imgflip.com/80d83h.jpg' width=\"400\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
